{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKoyeEgx3154"
   },
   "source": [
    "# Capstone : IT Ticket Classification [SEPT SUN GRP 4B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lN4eMOr93156"
   },
   "source": [
    "![1_yK5G9nHmOD-wrJSRSvPEpw.jpeg](attachment:1_yK5G9nHmOD-wrJSRSvPEpw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vGcFAxk3158"
   },
   "source": [
    "# Aim: Automatic Ticket Assignment [Part 5/6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSRvSAII3159"
   },
   "source": [
    "\n",
    "Build a classifier that can classify the tickets by analyzing text. Classify incidents to right functional groups can help organizations to reduce the resolving time of the issue and can focus on more productive tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "i0PyUpCV315-"
   },
   "source": [
    "## 5/6: Model Building with Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "5C4QNf1U315_"
   },
   "source": [
    "- Building a model architecture which can classify\n",
    "- Trying different model architectures by researching state of the art for similar tasks\n",
    "- Train the model\n",
    "- To deal with large training time, save the weights so that we can use them when training the model for the second time without starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K5ylEv2m316A"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3327,
     "status": "ok",
     "timestamp": 1598765923078,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "smqtw18W316B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re \n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "import pandas.testing as tm\n",
    "import spacy  \n",
    "#import langid \n",
    "import nltk\n",
    "\n",
    "import re, string\n",
    "from wordcloud import WordCloud\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.textcat import TextCat  \n",
    "from dateutil import parser\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "#from googletrans import Translator\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim \n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "#import enchant\n",
    "#from enchant.checker import SpellChecker\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D,GRU,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import logging, warnings\n",
    "from PIL import Image\n",
    "\n",
    "##--------------------##\n",
    "\n",
    "if sys.version_info >= (3, 3) :\n",
    "    import collections.abc as collections_abc\n",
    "else:\n",
    "    import collections as collections_abc\n",
    "    \n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "\n",
    "##-------------------##\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "#warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1598765927606,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "qW5VceCX316G",
    "outputId": "5c2155a9-f268-40f5-84ea-7d7b96798740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1.18.5', '1.0.5', '2.3.0', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__ , pd.__version__,tf.__version__, print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3184,
     "status": "ok",
     "timestamp": 1598765932576,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "nD-XHW_i316J",
    "outputId": "b545dceb-2900-4b7a-ccb1-1477b490803b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Download required NLTK stopwords corpus if it has not already been downloaded.\n",
    "Download required NLTK corpora if they have not already been downloaded.\n",
    "Download the NLTK averaged perceptron tagger that is required for this algorithm to run only if the corpora has not already been downloaded.\n",
    "Download Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences\n",
    "\"\"\"\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_XdUtZu316M"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23467,
     "status": "ok",
     "timestamp": 1598766017989,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "Qp06lrkE316N",
    "outputId": "838e133f-a6be-477d-d63c-ed91c5339c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "project_path = '/content/drive/My Drive/Colab Notebooks/GLCapstone/'\n",
    "file_name ='itsupportdatacleaned_3.csv'\n",
    "\n",
    "#\"\"\"\n",
    "#project_path = 'C:/MyDrive/Drive2/PersonalDrive/MachineLearning/GreatLearning/CapstoneProject/Final-Deck/'\n",
    "#file_name ='itsupportdatacleaned_3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2053,
     "status": "ok",
     "timestamp": 1598766023111,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "wPgj8fCP316Q",
    "outputId": "7c4e4955-78a2-4601-cbf9-968cc0fdbd55"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Short description</th>\n",
       "      <td>collaboration_platform site is not opening</td>\n",
       "      <td>account locked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Description</th>\n",
       "      <td>collaboration platform site open collaboration...</td>\n",
       "      <td>account lock account lock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caller</th>\n",
       "      <td>anpocezt qturbxsg</td>\n",
       "      <td>bkyphsgq cfyksehu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assignment group</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>GRP_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New_Assignment_Groups</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text_length</th>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <td>0.5092</td>\n",
       "      <td>0.9104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       0                          1\n",
       "Short description             collaboration_platform site is not opening            account locked.\n",
       "Description            collaboration platform site open collaboration...  account lock account lock\n",
       "Caller                                                 anpocezt qturbxsg          bkyphsgq cfyksehu\n",
       "Assignment group                                                   GRP_0                      GRP_0\n",
       "New_Assignment_Groups                                                  0                          0\n",
       "Text_length                                                           42                         15\n",
       "Dominant_Topic                                                         0                          0\n",
       "Topic_Perc_Contrib                                                0.5092                     0.9104"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(project_path+file_name,encoding=sys.getfilesystemencoding()) \n",
    "data.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1598766028559,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "24mhV2xO316U"
   },
   "outputs": [],
   "source": [
    "data.dropna(subset=[data.columns[1]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1598766030365,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "NXMQ6cN0316X",
    "outputId": "1f45d69a-2feb-41fd-9537-12df794bc9b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Short description</th>\n",
       "      <td>collaboration_platform site is not opening</td>\n",
       "      <td>account locked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Description</th>\n",
       "      <td>collaboration platform site open collaboration...</td>\n",
       "      <td>account lock account lock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caller</th>\n",
       "      <td>anpocezt qturbxsg</td>\n",
       "      <td>bkyphsgq cfyksehu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assignment group</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>GRP_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New_Assignment_Groups</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text_length</th>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <td>0.5092</td>\n",
       "      <td>0.9104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       0                          1\n",
       "Short description             collaboration_platform site is not opening            account locked.\n",
       "Description            collaboration platform site open collaboration...  account lock account lock\n",
       "Caller                                                 anpocezt qturbxsg          bkyphsgq cfyksehu\n",
       "Assignment group                                                   GRP_0                      GRP_0\n",
       "New_Assignment_Groups                                                  0                          0\n",
       "Text_length                                                           42                         15\n",
       "Dominant_Topic                                                         0                          0\n",
       "Topic_Perc_Contrib                                                0.5092                     0.9104"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1598766034409,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "kj6KWVPa316d",
    "outputId": "13bccf90-16e1-4061-c2d4-bdbd4ddecc61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46991, 8), 'Description')"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1598766036706,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "CmN3_J7Q316h"
   },
   "outputs": [],
   "source": [
    "data_3_resampled = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1598766039403,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "z6g76Hb0316l"
   },
   "outputs": [],
   "source": [
    "def wordTokenizer(dataframe):\n",
    "    tokenizer = Tokenizer(num_words=numWords,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=' ', char_level=False)\n",
    "    tokenizer.fit_on_texts(dataframe)\n",
    "    dataframe = tokenizer.texts_to_sequences(dataframe)\n",
    "    return tokenizer,dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1598766042511,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "fuVjyvhk316p"
   },
   "outputs": [],
   "source": [
    "data_3_resampled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_3_resampled.columns[1],data_3_resampled.columns[4]\n",
    "\n",
    "maxlen = 150\n",
    "numWords = 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDyB0P3r316s"
   },
   "source": [
    "# ML Model 1 Training :  Random Forest Classifier (Dataset 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yiBhBOs316t"
   },
   "source": [
    "Note: Tree based models work by learning in hierarchical manner. Random forests is a supervised learning algorithm which can be used both for classification as well as regression. It is also the most flexible and easy to use algorithm. It creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14037,
     "status": "ok",
     "timestamp": 1598766066569,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "TW5Kt_zo316u",
    "outputId": "458296e9-83da-4580-f6f7-5ef42331b2ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train RFC on dataset 3 : 9.933714628219604\n",
      "Accuracy: 0.9494627087988083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7928    0.6241    0.6984       141\n",
      "           1     0.8881    1.0000    0.9407       119\n",
      "           2     0.9821    0.8271    0.8980       133\n",
      "           3     1.0000    1.0000    1.0000       128\n",
      "           4     0.9091    0.8759    0.8922       137\n",
      "           5     0.9365    0.9593    0.9478       123\n",
      "           6     0.9746    0.9746    0.9746       118\n",
      "           7     1.0000    1.0000    1.0000       137\n",
      "           8     0.9786    1.0000    0.9892       137\n",
      "           9     0.9932    1.0000    0.9966       145\n",
      "          10     0.9508    0.9431    0.9469       123\n",
      "          11     0.9350    0.9504    0.9426       121\n",
      "          12     0.9286    0.9369    0.9327       111\n",
      "          13     0.9916    1.0000    0.9958       118\n",
      "          14     1.0000    1.0000    1.0000       132\n",
      "          15     1.0000    1.0000    1.0000       129\n",
      "          16     0.9783    1.0000    0.9890       135\n",
      "          17     0.9348    0.9627    0.9485       134\n",
      "          18     1.0000    0.9917    0.9958       120\n",
      "          19     1.0000    1.0000    1.0000       139\n",
      "          20     0.9922    1.0000    0.9961       128\n",
      "          21     1.0000    1.0000    1.0000       131\n",
      "          22     0.9776    0.9776    0.9776       134\n",
      "          23     0.9417    0.9187    0.9300       123\n",
      "          24     0.9861    0.8987    0.9404        79\n",
      "          25     0.9481    0.9624    0.9552       133\n",
      "          26     1.0000    1.0000    1.0000       125\n",
      "          27     0.9771    0.9922    0.9846       129\n",
      "          28     0.9844    0.9767    0.9805       129\n",
      "          29     1.0000    1.0000    1.0000       133\n",
      "          30     1.0000    1.0000    1.0000       114\n",
      "          31     1.0000    1.0000    1.0000       147\n",
      "          32     1.0000    1.0000    1.0000       132\n",
      "          33     0.9748    1.0000    0.9872       116\n",
      "          34     0.9811    0.9541    0.9674       109\n",
      "          35     1.0000    1.0000    1.0000       133\n",
      "          36     1.0000    1.0000    1.0000       149\n",
      "          37     0.9925    1.0000    0.9962       132\n",
      "          38     1.0000    1.0000    1.0000       146\n",
      "          39     0.9922    0.9407    0.9658       135\n",
      "          40     0.9902    0.8279    0.9018       122\n",
      "          41     1.0000    1.0000    1.0000       133\n",
      "          42     0.7000    0.9573    0.8087       117\n",
      "          43     0.7805    1.0000    0.8767        32\n",
      "          44     1.0000    1.0000    1.0000       134\n",
      "          45     0.9417    0.6879    0.7951       141\n",
      "          46     0.9922    1.0000    0.9961       127\n",
      "          47     1.0000    1.0000    1.0000       109\n",
      "          48     1.0000    1.0000    1.0000       141\n",
      "          49     1.0000    1.0000    1.0000       114\n",
      "          50     1.0000    1.0000    1.0000       134\n",
      "          51     1.0000    1.0000    1.0000       122\n",
      "          52     1.0000    1.0000    1.0000       138\n",
      "          53     1.0000    0.5390    0.7005       141\n",
      "          54     1.0000    1.0000    1.0000       140\n",
      "          55     1.0000    1.0000    1.0000       133\n",
      "          56     0.9155    0.4815    0.6311       135\n",
      "          57     0.9732    0.8385    0.9008       130\n",
      "          58     1.0000    1.0000    1.0000       127\n",
      "          59     1.0000    1.0000    1.0000       107\n",
      "          60     1.0000    1.0000    1.0000       117\n",
      "          61     1.0000    1.0000    1.0000       131\n",
      "          62     1.0000    1.0000    1.0000       127\n",
      "          63     1.0000    1.0000    1.0000       149\n",
      "          64     1.0000    1.0000    1.0000       126\n",
      "          65     1.0000    1.0000    1.0000       126\n",
      "          66     1.0000    1.0000    1.0000       120\n",
      "          67     0.9926    1.0000    0.9963       135\n",
      "          68     1.0000    1.0000    1.0000       126\n",
      "          69     1.0000    1.0000    1.0000       128\n",
      "          70     0.9624    1.0000    0.9808       128\n",
      "          71     1.0000    1.0000    1.0000       122\n",
      "          72     0.9851    0.5280    0.6875       125\n",
      "          73     0.2869    0.8400    0.4277       125\n",
      "\n",
      "    accuracy                         0.9495      9399\n",
      "   macro avg     0.9654    0.9509    0.9523      9399\n",
      "weighted avg     0.9679    0.9495    0.9528      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  accuracy\n",
       "1  Random Forest  0.949463"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "data_3_resampled_RFC=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_RFC.fit(X_train,y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train RFC on dataset 3 : {end - start}\")\n",
    "\n",
    "\n",
    "y_pred=data_3_resampled_RFC.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_RFC.predict(X_test), digits=4))\n",
    "\n",
    "results_ML_Models = pd.DataFrame()\n",
    "results_data_3_resampled_RFC = pd.DataFrame({'Model':['Random Forest'], 'accuracy': [metrics.accuracy_score(y_test, y_pred)]},index={'1'})\n",
    "results_ML_Models = pd.concat([results_ML_Models, results_data_3_resampled_RFC])\n",
    "results_ML_Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aS5bZC20316y"
   },
   "source": [
    "#  ML Model 2 : Random Forest Classifier (Weighted, Dataset 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuPnmfpF316z"
   },
   "source": [
    "Note: We have received good accuracy using RFC. Also there is not much difference between with and without assigning the class weights which we plan to explore further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21671,
     "status": "ok",
     "timestamp": 1598766102137,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "fDu_Sy9B3160",
    "outputId": "b6df8eb9-fdd7-45b8-8255-a790376e3a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train RFC Weighted on dataset 3: 17.530489683151245\n",
      "Accuracy: 0.9497818916906053\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8426    0.6454    0.7309       141\n",
      "           1     0.8881    1.0000    0.9407       119\n",
      "           2     0.9649    0.8271    0.8907       133\n",
      "           3     1.0000    1.0000    1.0000       128\n",
      "           4     0.9453    0.8832    0.9132       137\n",
      "           5     0.9219    0.9593    0.9402       123\n",
      "           6     0.9829    0.9746    0.9787       118\n",
      "           7     1.0000    1.0000    1.0000       137\n",
      "           8     0.9856    1.0000    0.9928       137\n",
      "           9     0.9932    1.0000    0.9966       145\n",
      "          10     0.9748    0.9431    0.9587       123\n",
      "          11     0.9504    0.9504    0.9504       121\n",
      "          12     0.8595    0.9369    0.8966       111\n",
      "          13     1.0000    1.0000    1.0000       118\n",
      "          14     1.0000    1.0000    1.0000       132\n",
      "          15     1.0000    1.0000    1.0000       129\n",
      "          16     0.9854    1.0000    0.9926       135\n",
      "          17     0.9214    0.9627    0.9416       134\n",
      "          18     0.9917    0.9917    0.9917       120\n",
      "          19     1.0000    1.0000    1.0000       139\n",
      "          20     0.9922    1.0000    0.9961       128\n",
      "          21     1.0000    1.0000    1.0000       131\n",
      "          22     0.9776    0.9776    0.9776       134\n",
      "          23     0.9576    0.9187    0.9378       123\n",
      "          24     0.9861    0.8987    0.9404        79\n",
      "          25     0.9481    0.9624    0.9552       133\n",
      "          26     1.0000    1.0000    1.0000       125\n",
      "          27     0.9697    0.9922    0.9808       129\n",
      "          28     0.9921    0.9767    0.9844       129\n",
      "          29     1.0000    1.0000    1.0000       133\n",
      "          30     1.0000    1.0000    1.0000       114\n",
      "          31     1.0000    1.0000    1.0000       147\n",
      "          32     1.0000    1.0000    1.0000       132\n",
      "          33     0.9748    1.0000    0.9872       116\n",
      "          34     0.9905    0.9541    0.9720       109\n",
      "          35     0.9925    1.0000    0.9963       133\n",
      "          36     1.0000    1.0000    1.0000       149\n",
      "          37     1.0000    1.0000    1.0000       132\n",
      "          38     1.0000    1.0000    1.0000       146\n",
      "          39     0.9922    0.9407    0.9658       135\n",
      "          40     0.9902    0.8279    0.9018       122\n",
      "          41     1.0000    1.0000    1.0000       133\n",
      "          42     0.7000    0.9573    0.8087       117\n",
      "          43     0.7805    1.0000    0.8767        32\n",
      "          44     1.0000    1.0000    1.0000       134\n",
      "          45     0.9417    0.6879    0.7951       141\n",
      "          46     0.9922    1.0000    0.9961       127\n",
      "          47     1.0000    1.0000    1.0000       109\n",
      "          48     0.9930    1.0000    0.9965       141\n",
      "          49     1.0000    1.0000    1.0000       114\n",
      "          50     1.0000    1.0000    1.0000       134\n",
      "          51     1.0000    1.0000    1.0000       122\n",
      "          52     1.0000    1.0000    1.0000       138\n",
      "          53     1.0000    0.5390    0.7005       141\n",
      "          54     1.0000    1.0000    1.0000       140\n",
      "          55     1.0000    1.0000    1.0000       133\n",
      "          56     0.9028    0.4815    0.6280       135\n",
      "          57     0.9732    0.8385    0.9008       130\n",
      "          58     1.0000    1.0000    1.0000       127\n",
      "          59     1.0000    1.0000    1.0000       107\n",
      "          60     1.0000    1.0000    1.0000       117\n",
      "          61     1.0000    1.0000    1.0000       131\n",
      "          62     1.0000    1.0000    1.0000       127\n",
      "          63     1.0000    1.0000    1.0000       149\n",
      "          64     1.0000    1.0000    1.0000       126\n",
      "          65     1.0000    1.0000    1.0000       126\n",
      "          66     1.0000    1.0000    1.0000       120\n",
      "          67     0.9926    1.0000    0.9963       135\n",
      "          68     1.0000    1.0000    1.0000       126\n",
      "          69     1.0000    1.0000    1.0000       128\n",
      "          70     0.9624    1.0000    0.9808       128\n",
      "          71     1.0000    1.0000    1.0000       122\n",
      "          72     0.9701    0.5200    0.6771       125\n",
      "          73     0.2861    0.8400    0.4268       125\n",
      "\n",
      "    accuracy                         0.9498      9399\n",
      "   macro avg     0.9658    0.9512    0.9526      9399\n",
      "weighted avg     0.9683    0.9498    0.9532      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "data_3_resampled_RFC_W = RandomForestClassifier(n_estimators=100, criterion = 'entropy', class_weight='balanced') # Class Weights\n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_RFC_W.fit(X_train,y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train RFC Weighted on dataset 3: {end - start}\")\n",
    "\n",
    "y_pred=data_3_resampled_RFC_W.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_RFC_W.predict(X_test), digits=4))\n",
    "\n",
    "#results_ML_Models = pd.DataFrame()\n",
    "results_data_3_resampled_RFC_W = pd.DataFrame({'Model':['Random Forest Classifier - Weighted'], 'accuracy': [metrics.accuracy_score(y_test, y_pred)]},index={'1'})\n",
    "results_ML_Models = pd.concat([results_ML_Models, results_data_3_resampled_RFC_W])\n",
    "results_ML_Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXW0Un433164"
   },
   "source": [
    "#  ML Model 3 : Adaboost (Dataset 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33998,
     "status": "ok",
     "timestamp": 1598766144852,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "f19aL-SM3164",
    "outputId": "d3ec27ae-d1c2-49e9-84cd-370553a5ed1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train Adaboost on dataset 3 : 25.727181434631348\n",
      "Accuracy: 0.08234918608362592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0412    0.0709    0.0521       141\n",
      "           1     0.0000    0.0000    0.0000       119\n",
      "           2     0.7556    0.2556    0.3820       133\n",
      "           3     0.0000    0.0000    0.0000       128\n",
      "           4     0.0452    0.0657    0.0536       137\n",
      "           5     0.0351    0.0163    0.0222       123\n",
      "           6     0.0169    0.0593    0.0263       118\n",
      "           7     0.0178    0.0584    0.0273       137\n",
      "           8     0.0000    0.0000    0.0000       137\n",
      "           9     0.5833    0.0483    0.0892       145\n",
      "          10     0.0784    0.0325    0.0460       123\n",
      "          11     0.0000    0.0000    0.0000       121\n",
      "          12     0.0195    0.0450    0.0272       111\n",
      "          13     0.0259    0.0254    0.0256       118\n",
      "          14     0.0314    0.0379    0.0344       132\n",
      "          15     0.0510    0.1008    0.0677       129\n",
      "          16     0.0000    0.0000    0.0000       135\n",
      "          17     0.0000    0.0000    0.0000       134\n",
      "          18     0.3750    0.0250    0.0469       120\n",
      "          19     0.0215    0.0504    0.0302       139\n",
      "          20     0.0000    0.0000    0.0000       128\n",
      "          21     0.0877    0.1145    0.0993       131\n",
      "          22     0.0079    0.0448    0.0135       134\n",
      "          23     0.0386    0.0732    0.0506       123\n",
      "          24     0.0000    0.0000    0.0000        79\n",
      "          25     0.0000    0.0000    0.0000       133\n",
      "          26     0.0000    0.0000    0.0000       125\n",
      "          27     0.0000    0.0000    0.0000       129\n",
      "          28     0.0000    0.0000    0.0000       129\n",
      "          29     0.0000    0.0000    0.0000       133\n",
      "          30     0.0000    0.0000    0.0000       114\n",
      "          31     0.1478    0.3741    0.2119       147\n",
      "          32     0.0000    0.0000    0.0000       132\n",
      "          33     0.0268    0.0603    0.0371       116\n",
      "          34     0.0000    0.0000    0.0000       109\n",
      "          35     0.0374    0.0301    0.0333       133\n",
      "          36     0.0629    0.2148    0.0973       149\n",
      "          37     0.0000    0.0000    0.0000       132\n",
      "          38     0.0000    0.0000    0.0000       146\n",
      "          39     0.2439    0.0741    0.1136       135\n",
      "          40     0.0000    0.0000    0.0000       122\n",
      "          41     0.0712    0.8496    0.1313       133\n",
      "          42     0.0000    0.0000    0.0000       117\n",
      "          43     0.0000    0.0000    0.0000        32\n",
      "          44     0.0000    0.0000    0.0000       134\n",
      "          45     0.0000    0.0000    0.0000       141\n",
      "          46     0.1070    0.2283    0.1457       127\n",
      "          47     0.0000    0.0000    0.0000       109\n",
      "          48     0.0000    0.0000    0.0000       141\n",
      "          49     0.0000    0.0000    0.0000       114\n",
      "          50     0.0000    0.0000    0.0000       134\n",
      "          51     0.0884    0.4918    0.1498       122\n",
      "          52     1.0000    0.2826    0.4407       138\n",
      "          53     0.0000    0.0000    0.0000       141\n",
      "          54     0.0000    0.0000    0.0000       140\n",
      "          55     0.0000    0.0000    0.0000       133\n",
      "          56     0.0000    0.0000    0.0000       135\n",
      "          57     0.0000    0.0000    0.0000       130\n",
      "          58     0.0000    0.0000    0.0000       127\n",
      "          59     0.0000    0.0000    0.0000       107\n",
      "          60     0.5159    0.6923    0.5912       117\n",
      "          61     0.0000    0.0000    0.0000       131\n",
      "          62     0.2041    0.0787    0.1136       127\n",
      "          63     0.0000    0.0000    0.0000       149\n",
      "          64     0.9767    1.0000    0.9882       126\n",
      "          65     0.0000    0.0000    0.0000       126\n",
      "          66     0.0000    0.0000    0.0000       120\n",
      "          67     0.0423    0.1852    0.0689       135\n",
      "          68     0.0000    0.0000    0.0000       126\n",
      "          69     0.0000    0.0000    0.0000       128\n",
      "          70     0.0000    0.0000    0.0000       128\n",
      "          71     0.0000    0.0000    0.0000       122\n",
      "          72     0.1014    0.2880    0.1500       125\n",
      "          73     0.0000    0.0000    0.0000       125\n",
      "\n",
      "    accuracy                         0.0823      9399\n",
      "   macro avg     0.0792    0.0807    0.0590      9399\n",
      "weighted avg     0.0818    0.0823    0.0601      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "data_3_resampled_Ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, algorithm='SAMME.R', random_state=22) \n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_Ada.fit(X_train,y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train Adaboost on dataset 3 : {end - start}\")\n",
    "\n",
    "y_pred=data_3_resampled_Ada.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_Ada.predict(X_test), digits=4))\n",
    "\n",
    "#results_ML_Models = pd.DataFrame()\n",
    "results_data_3_resampled_Ada= pd.DataFrame({'Model':['Adaboost Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred)]},index={'1'})\n",
    "results_ML_Models = pd.concat([results_ML_Models, results_data_3_resampled_Ada])\n",
    "results_ML_Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aluXitmb3168"
   },
   "source": [
    "#  ML Model 4 : Bagging (Dataset 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 230621,
     "status": "ok",
     "timestamp": 1598766380196,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "oD95Vc4A3168",
    "outputId": "9a83fd53-0e1a-49d6-98b9-4c59a2eb282a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train Bagging on dataset 3 : 222.9662311077118\n",
      "Accuracy: 0.947015639961698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8608    0.4823    0.6182       141\n",
      "           1     0.8815    1.0000    0.9370       119\n",
      "           2     0.9735    0.8271    0.8943       133\n",
      "           3     0.9846    1.0000    0.9922       128\n",
      "           4     0.9370    0.8686    0.9015       137\n",
      "           5     0.9516    0.9593    0.9555       123\n",
      "           6     0.9829    0.9746    0.9787       118\n",
      "           7     0.9786    1.0000    0.9892       137\n",
      "           8     0.9580    1.0000    0.9786       137\n",
      "           9     0.9667    1.0000    0.9831       145\n",
      "          10     0.9831    0.9431    0.9627       123\n",
      "          11     0.8779    0.9504    0.9127       121\n",
      "          12     0.9444    0.9189    0.9315       111\n",
      "          13     1.0000    1.0000    1.0000       118\n",
      "          14     1.0000    1.0000    1.0000       132\n",
      "          15     1.0000    1.0000    1.0000       129\n",
      "          16     0.9783    1.0000    0.9890       135\n",
      "          17     0.9692    0.9403    0.9545       134\n",
      "          18     0.9675    0.9917    0.9794       120\n",
      "          19     0.9653    1.0000    0.9823       139\n",
      "          20     0.9922    1.0000    0.9961       128\n",
      "          21     1.0000    1.0000    1.0000       131\n",
      "          22     0.9850    0.9776    0.9813       134\n",
      "          23     0.9504    0.9350    0.9426       123\n",
      "          24     0.9861    0.8987    0.9404        79\n",
      "          25     0.9481    0.9624    0.9552       133\n",
      "          26     0.9921    1.0000    0.9960       125\n",
      "          27     0.9697    0.9922    0.9808       129\n",
      "          28     0.9921    0.9767    0.9844       129\n",
      "          29     0.9852    1.0000    0.9925       133\n",
      "          30     0.9913    1.0000    0.9956       114\n",
      "          31     1.0000    1.0000    1.0000       147\n",
      "          32     1.0000    1.0000    1.0000       132\n",
      "          33     0.9748    1.0000    0.9872       116\n",
      "          34     0.9905    0.9541    0.9720       109\n",
      "          35     1.0000    1.0000    1.0000       133\n",
      "          36     1.0000    1.0000    1.0000       149\n",
      "          37     0.9925    1.0000    0.9962       132\n",
      "          38     1.0000    1.0000    1.0000       146\n",
      "          39     0.9845    0.9407    0.9621       135\n",
      "          40     0.9712    0.8279    0.8938       122\n",
      "          41     1.0000    1.0000    1.0000       133\n",
      "          42     0.6957    0.9573    0.8058       117\n",
      "          43     0.7805    1.0000    0.8767        32\n",
      "          44     1.0000    1.0000    1.0000       134\n",
      "          45     0.9423    0.6950    0.8000       141\n",
      "          46     0.9769    1.0000    0.9883       127\n",
      "          47     0.9820    1.0000    0.9909       109\n",
      "          48     0.9930    1.0000    0.9965       141\n",
      "          49     1.0000    1.0000    1.0000       114\n",
      "          50     1.0000    1.0000    1.0000       134\n",
      "          51     0.9839    1.0000    0.9919       122\n",
      "          52     1.0000    1.0000    1.0000       138\n",
      "          53     0.9870    0.5390    0.6972       141\n",
      "          54     1.0000    1.0000    1.0000       140\n",
      "          55     0.9925    1.0000    0.9963       133\n",
      "          56     0.9155    0.4815    0.6311       135\n",
      "          57     0.9561    0.8385    0.8934       130\n",
      "          58     1.0000    1.0000    1.0000       127\n",
      "          59     1.0000    1.0000    1.0000       107\n",
      "          60     1.0000    1.0000    1.0000       117\n",
      "          61     1.0000    1.0000    1.0000       131\n",
      "          62     1.0000    1.0000    1.0000       127\n",
      "          63     1.0000    1.0000    1.0000       149\n",
      "          64     1.0000    1.0000    1.0000       126\n",
      "          65     1.0000    1.0000    1.0000       126\n",
      "          66     0.9917    1.0000    0.9959       120\n",
      "          67     0.9854    1.0000    0.9926       135\n",
      "          68     1.0000    1.0000    1.0000       126\n",
      "          69     1.0000    1.0000    1.0000       128\n",
      "          70     0.9552    1.0000    0.9771       128\n",
      "          71     1.0000    1.0000    1.0000       122\n",
      "          72     0.9565    0.5280    0.6804       125\n",
      "          73     0.2853    0.8400    0.4260       125\n",
      "\n",
      "    accuracy                         0.9470      9399\n",
      "   macro avg     0.9628    0.9487    0.9494      9399\n",
      "weighted avg     0.9652    0.9470    0.9498      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.947016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349\n",
       "1                   Bagging Classifier  0.947016"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "data_3_resampled_Bagg= BaggingClassifier(n_estimators=200, oob_score=True, bootstrap=True, random_state=22) \n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_Bagg.fit(X_train,y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train Bagging on dataset 3 : {end - start}\")\n",
    "\n",
    "y_pred=data_3_resampled_Bagg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_Bagg.predict(X_test), digits=4))\n",
    "\n",
    "#results_ML_Models = pd.DataFrame()\n",
    "results_data_3_resampled_Bagg = pd.DataFrame({'Model':['Bagging Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred)]},index={'1'})\n",
    "results_ML_Models = pd.concat([results_ML_Models, results_data_3_resampled_Bagg])\n",
    "results_ML_Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVV-fw3H317B"
   },
   "source": [
    "#  ML Model 5 : GradientBoost (Dataset 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2594933,
     "status": "ok",
     "timestamp": 1598768985646,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "e5wEdOhq317B",
    "outputId": "ecf05f48-a12e-4ec2-e32c-9f621682b667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train Bagging on dataset 3 : 2589.002761363983\n",
      "Accuracy: 0.8744547292265135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5281    0.3333    0.4087       141\n",
      "           1     0.8380    1.0000    0.9119       119\n",
      "           2     0.9263    0.6617    0.7719       133\n",
      "           3     0.8312    1.0000    0.9078       128\n",
      "           4     0.8000    0.4964    0.6126       137\n",
      "           5     0.8500    0.6911    0.7623       123\n",
      "           6     0.7218    0.8136    0.7649       118\n",
      "           7     0.9041    0.9635    0.9329       137\n",
      "           8     0.7973    0.8613    0.8281       137\n",
      "           9     0.9539    1.0000    0.9764       145\n",
      "          10     0.7176    0.7642    0.7402       123\n",
      "          11     0.6829    0.4628    0.5517       121\n",
      "          12     0.7901    0.5766    0.6667       111\n",
      "          13     0.9048    0.9661    0.9344       118\n",
      "          14     0.9296    1.0000    0.9635       132\n",
      "          15     0.8414    0.9457    0.8905       129\n",
      "          16     0.8599    1.0000    0.9247       135\n",
      "          17     0.7750    0.6940    0.7323       134\n",
      "          18     0.6903    0.6500    0.6695       120\n",
      "          19     0.8623    0.8561    0.8592       139\n",
      "          20     0.9078    1.0000    0.9517       128\n",
      "          21     0.8993    0.9542    0.9259       131\n",
      "          22     0.8958    0.6418    0.7478       134\n",
      "          23     0.6341    0.4228    0.5073       123\n",
      "          24     0.8659    0.8987    0.8820        79\n",
      "          25     0.6146    0.8872    0.7262       133\n",
      "          26     1.0000    1.0000    1.0000       125\n",
      "          27     0.8030    0.8217    0.8123       129\n",
      "          28     0.9091    0.8527    0.8800       129\n",
      "          29     1.0000    1.0000    1.0000       133\n",
      "          30     0.9344    1.0000    0.9661       114\n",
      "          31     0.9423    1.0000    0.9703       147\n",
      "          32     1.0000    1.0000    1.0000       132\n",
      "          33     0.9206    1.0000    0.9587       116\n",
      "          34     0.7212    0.6881    0.7042       109\n",
      "          35     0.8217    0.9699    0.8897       133\n",
      "          36     0.8889    0.9664    0.9260       149\n",
      "          37     0.9429    1.0000    0.9706       132\n",
      "          38     0.9865    1.0000    0.9932       146\n",
      "          39     0.9478    0.9407    0.9442       135\n",
      "          40     0.9099    0.8279    0.8670       122\n",
      "          41     1.0000    1.0000    1.0000       133\n",
      "          42     0.6364    0.9573    0.7645       117\n",
      "          43     1.0000    0.8438    0.9153        32\n",
      "          44     1.0000    1.0000    1.0000       134\n",
      "          45     0.9479    0.6454    0.7679       141\n",
      "          46     0.9203    1.0000    0.9585       127\n",
      "          47     0.9732    1.0000    0.9864       109\n",
      "          48     0.9930    1.0000    0.9965       141\n",
      "          49     0.9344    1.0000    0.9661       114\n",
      "          50     1.0000    1.0000    1.0000       134\n",
      "          51     0.9919    1.0000    0.9959       122\n",
      "          52     1.0000    1.0000    1.0000       138\n",
      "          53     1.0000    0.5390    0.7005       141\n",
      "          54     1.0000    1.0000    1.0000       140\n",
      "          55     1.0000    1.0000    1.0000       133\n",
      "          56     0.8421    0.2370    0.3699       135\n",
      "          57     0.9732    0.8385    0.9008       130\n",
      "          58     1.0000    1.0000    1.0000       127\n",
      "          59     0.9727    1.0000    0.9862       107\n",
      "          60     1.0000    1.0000    1.0000       117\n",
      "          61     1.0000    1.0000    1.0000       131\n",
      "          62     0.9769    1.0000    0.9883       127\n",
      "          63     1.0000    1.0000    1.0000       149\n",
      "          64     1.0000    1.0000    1.0000       126\n",
      "          65     1.0000    1.0000    1.0000       126\n",
      "          66     1.0000    1.0000    1.0000       120\n",
      "          67     0.8356    0.9037    0.8683       135\n",
      "          68     1.0000    1.0000    1.0000       126\n",
      "          69     1.0000    1.0000    1.0000       128\n",
      "          70     0.9624    1.0000    0.9808       128\n",
      "          71     1.0000    1.0000    1.0000       122\n",
      "          72     0.7826    0.4320    0.5567       125\n",
      "          73     0.2479    0.6960    0.3655       125\n",
      "\n",
      "    accuracy                         0.8745      9399\n",
      "   macro avg     0.8884    0.8743    0.8716      9399\n",
      "weighted avg     0.8889    0.8745    0.8718      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.947016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoost Classifier</td>\n",
       "      <td>0.874455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349\n",
       "1                   Bagging Classifier  0.947016\n",
       "1             GradientBoost Classifier  0.874455"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "data_3_resampled_gd= GradientBoostingClassifier(n_estimators=100, learning_rate = 0.05, random_state=22) \n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_gd.fit(X_train,y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train Bagging on dataset 3 : {end - start}\")\n",
    "\n",
    "y_pred=data_3_resampled_gd.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_gd.predict(X_test), digits=4))\n",
    "\n",
    "#results_ML_Models = pd.DataFrame()\n",
    "results_data_3_resampled_gd= pd.DataFrame({'Model':['GradientBoost Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred)]},index={'1'})\n",
    "results_ML_Models  = pd.concat([results_ML_Models, results_data_3_resampled_gd])\n",
    "results_ML_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 905547,
     "status": "ok",
     "timestamp": 1598775198166,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "DADO-5qs317E",
    "outputId": "e50ee59b-6bd2-4b6b-9181-d8d9d91cb528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train XGBoost on dataset 3 : 881.4950704574585\n",
      "Accuracy: 0.8822215129269071\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5233    0.3191    0.3965       141\n",
      "           1     0.8156    0.9664    0.8846       119\n",
      "           2     0.9186    0.5940    0.7215       133\n",
      "           3     0.9275    1.0000    0.9624       128\n",
      "           4     0.8242    0.5474    0.6579       137\n",
      "           5     0.8165    0.7236    0.7672       123\n",
      "           6     0.7857    0.7458    0.7652       118\n",
      "           7     0.8618    0.9562    0.9066       137\n",
      "           8     0.8880    0.8102    0.8473       137\n",
      "           9     0.9603    1.0000    0.9797       145\n",
      "          10     0.8362    0.7886    0.8117       123\n",
      "          11     0.6444    0.4793    0.5498       121\n",
      "          12     0.7423    0.6486    0.6923       111\n",
      "          13     0.8837    0.9661    0.9231       118\n",
      "          14     0.9231    1.0000    0.9600       132\n",
      "          15     0.8716    1.0000    0.9314       129\n",
      "          16     0.9507    1.0000    0.9747       135\n",
      "          17     0.7226    0.7388    0.7306       134\n",
      "          18     0.7627    0.7500    0.7563       120\n",
      "          19     0.9385    0.8777    0.9071       139\n",
      "          20     0.8533    1.0000    0.9209       128\n",
      "          21     0.8675    1.0000    0.9291       131\n",
      "          22     0.8718    0.7612    0.8127       134\n",
      "          23     0.6620    0.3821    0.4845       123\n",
      "          24     0.8636    0.9620    0.9102        79\n",
      "          25     0.6723    0.8947    0.7677       133\n",
      "          26     1.0000    1.0000    1.0000       125\n",
      "          27     0.8273    0.8915    0.8582       129\n",
      "          28     0.9744    0.8837    0.9268       129\n",
      "          29     1.0000    1.0000    1.0000       133\n",
      "          30     0.9421    1.0000    0.9702       114\n",
      "          31     0.9735    1.0000    0.9866       147\n",
      "          32     1.0000    1.0000    1.0000       132\n",
      "          33     0.9134    1.0000    0.9547       116\n",
      "          34     0.7959    0.7156    0.7536       109\n",
      "          35     0.8408    0.9925    0.9103       133\n",
      "          36     0.9586    0.9329    0.9456       149\n",
      "          37     0.8980    1.0000    0.9462       132\n",
      "          38     1.0000    1.0000    1.0000       146\n",
      "          39     0.9549    0.9407    0.9478       135\n",
      "          40     0.9151    0.7951    0.8509       122\n",
      "          41     1.0000    1.0000    1.0000       133\n",
      "          42     0.6829    0.9573    0.7972       117\n",
      "          43     1.0000    0.8438    0.9153        32\n",
      "          44     1.0000    1.0000    1.0000       134\n",
      "          45     0.8700    0.6170    0.7220       141\n",
      "          46     0.9549    1.0000    0.9769       127\n",
      "          47     0.9909    1.0000    0.9954       109\n",
      "          48     0.9724    1.0000    0.9860       141\n",
      "          49     0.9661    1.0000    0.9828       114\n",
      "          50     1.0000    1.0000    1.0000       134\n",
      "          51     0.9457    1.0000    0.9721       122\n",
      "          52     1.0000    1.0000    1.0000       138\n",
      "          53     1.0000    0.5390    0.7005       141\n",
      "          54     1.0000    1.0000    1.0000       140\n",
      "          55     1.0000    1.0000    1.0000       133\n",
      "          56     0.7800    0.2889    0.4216       135\n",
      "          57     0.9732    0.8385    0.9008       130\n",
      "          58     1.0000    1.0000    1.0000       127\n",
      "          59     0.9386    1.0000    0.9683       107\n",
      "          60     1.0000    1.0000    1.0000       117\n",
      "          61     1.0000    1.0000    1.0000       131\n",
      "          62     1.0000    1.0000    1.0000       127\n",
      "          63     1.0000    1.0000    1.0000       149\n",
      "          64     1.0000    1.0000    1.0000       126\n",
      "          65     1.0000    1.0000    1.0000       126\n",
      "          66     1.0000    1.0000    1.0000       120\n",
      "          67     0.7764    0.9259    0.8446       135\n",
      "          68     1.0000    1.0000    1.0000       126\n",
      "          69     1.0000    1.0000    1.0000       128\n",
      "          70     0.9624    1.0000    0.9808       128\n",
      "          71     1.0000    1.0000    1.0000       122\n",
      "          72     0.7326    0.5040    0.5972       125\n",
      "          73     0.2542    0.7280    0.3768       125\n",
      "\n",
      "    accuracy                         0.8822      9399\n",
      "   macro avg     0.8944    0.8825    0.8803      9399\n",
      "weighted avg     0.8950    0.8822    0.8803      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.947016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoost Classifier</td>\n",
       "      <td>0.874455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>0.882222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349\n",
       "1                   Bagging Classifier  0.947016\n",
       "1             GradientBoost Classifier  0.874455\n",
       "1                   XGBoost Classifier  0.882222"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "data_3_resampled_xg = XGBClassifier() \n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_xg.fit(X_train,y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train XGBoost on dataset 3 : {end - start}\")\n",
    "\n",
    "y_pred=data_3_resampled_xg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_xg.predict(X_test), digits=4))\n",
    "\n",
    "results_data_3_resampled_xg= pd.DataFrame({'Model':['XGBoost Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred)]},index={'1'})\n",
    "results_ML_Models  = pd.concat([results_ML_Models, results_data_3_resampled_xg])\n",
    "results_ML_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12478,
     "status": "ok",
     "timestamp": 1598775255987,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "0BP7xan3317H",
    "outputId": "a633c2d1-778e-42d7-f238-5314d92a1aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/86/c3dcb600b4f9e7584ed90ea9d30a717fb5c0111574675f442c3e7bc19535/catboost-0.24.1-cp36-none-manylinux1_x86_64.whl (66.1MB)\n",
      "\u001b[K     |████████████████████████████████| 66.1MB 64kB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.5)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
      "Installing collected packages: catboost\n",
      "Successfully installed catboost-0.24.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6016235,
     "status": "ok",
     "timestamp": 1598781299540,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "jaoNrKYF317J",
    "outputId": "0965989f-7bdc-496e-ad8e-d11258aa89f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Learning rate set to 0.095204\n",
      "0:\tlearn: 3.8768763\ttotal: 7.66s\tremaining: 2h 7m 30s\n",
      "1:\tlearn: 3.6623057\ttotal: 13.6s\tremaining: 1h 53m 2s\n",
      "2:\tlearn: 3.3778772\ttotal: 19.5s\tremaining: 1h 47m 58s\n",
      "3:\tlearn: 3.2286763\ttotal: 25.4s\tremaining: 1h 45m 25s\n",
      "4:\tlearn: 3.0328385\ttotal: 31.3s\tremaining: 1h 43m 47s\n",
      "5:\tlearn: 2.9334516\ttotal: 37.2s\tremaining: 1h 42m 47s\n",
      "6:\tlearn: 2.8381189\ttotal: 43.1s\tremaining: 1h 41m 57s\n",
      "7:\tlearn: 2.7431585\ttotal: 49.2s\tremaining: 1h 41m 37s\n",
      "8:\tlearn: 2.6572968\ttotal: 55.2s\tremaining: 1h 41m 13s\n",
      "9:\tlearn: 2.5576882\ttotal: 1m 1s\tremaining: 1h 40m 44s\n",
      "10:\tlearn: 2.4737256\ttotal: 1m 7s\tremaining: 1h 40m 25s\n",
      "11:\tlearn: 2.4365443\ttotal: 1m 12s\tremaining: 1h 40m 9s\n",
      "12:\tlearn: 2.3918789\ttotal: 1m 18s\tremaining: 1h 39m 53s\n",
      "13:\tlearn: 2.3120466\ttotal: 1m 24s\tremaining: 1h 39m 34s\n",
      "14:\tlearn: 2.2532903\ttotal: 1m 30s\tremaining: 1h 39m 18s\n",
      "15:\tlearn: 2.1968676\ttotal: 1m 36s\tremaining: 1h 39m 2s\n",
      "16:\tlearn: 2.1380402\ttotal: 1m 42s\tremaining: 1h 38m 47s\n",
      "17:\tlearn: 2.0722125\ttotal: 1m 48s\tremaining: 1h 38m 33s\n",
      "18:\tlearn: 2.0271289\ttotal: 1m 54s\tremaining: 1h 38m 19s\n",
      "19:\tlearn: 1.9985002\ttotal: 2m\tremaining: 1h 38m 13s\n",
      "20:\tlearn: 1.9679890\ttotal: 2m 6s\tremaining: 1h 38m 7s\n",
      "21:\tlearn: 1.9347029\ttotal: 2m 12s\tremaining: 1h 38m 2s\n",
      "22:\tlearn: 1.9160858\ttotal: 2m 18s\tremaining: 1h 37m 55s\n",
      "23:\tlearn: 1.8706910\ttotal: 2m 24s\tremaining: 1h 37m 44s\n",
      "24:\tlearn: 1.8160252\ttotal: 2m 30s\tremaining: 1h 37m 32s\n",
      "25:\tlearn: 1.7890840\ttotal: 2m 36s\tremaining: 1h 37m 25s\n",
      "26:\tlearn: 1.7624019\ttotal: 2m 42s\tremaining: 1h 37m 18s\n",
      "27:\tlearn: 1.7394294\ttotal: 2m 48s\tremaining: 1h 37m 12s\n",
      "28:\tlearn: 1.6981880\ttotal: 2m 53s\tremaining: 1h 37m 3s\n",
      "29:\tlearn: 1.6726549\ttotal: 2m 59s\tremaining: 1h 36m 57s\n",
      "30:\tlearn: 1.6432181\ttotal: 3m 5s\tremaining: 1h 36m 50s\n",
      "31:\tlearn: 1.6282611\ttotal: 3m 11s\tremaining: 1h 36m 43s\n",
      "32:\tlearn: 1.5885231\ttotal: 3m 17s\tremaining: 1h 36m 36s\n",
      "33:\tlearn: 1.5676458\ttotal: 3m 23s\tremaining: 1h 36m 28s\n",
      "34:\tlearn: 1.5466307\ttotal: 3m 29s\tremaining: 1h 36m 19s\n",
      "35:\tlearn: 1.5342599\ttotal: 3m 35s\tremaining: 1h 36m 15s\n",
      "36:\tlearn: 1.5051907\ttotal: 3m 41s\tremaining: 1h 36m 7s\n",
      "37:\tlearn: 1.4874986\ttotal: 3m 47s\tremaining: 1h 36m 1s\n",
      "38:\tlearn: 1.4772763\ttotal: 3m 53s\tremaining: 1h 35m 55s\n",
      "39:\tlearn: 1.4538154\ttotal: 3m 59s\tremaining: 1h 35m 49s\n",
      "40:\tlearn: 1.4411423\ttotal: 4m 5s\tremaining: 1h 35m 44s\n",
      "41:\tlearn: 1.4237274\ttotal: 4m 11s\tremaining: 1h 35m 38s\n",
      "42:\tlearn: 1.4063467\ttotal: 4m 17s\tremaining: 1h 35m 30s\n",
      "43:\tlearn: 1.3970528\ttotal: 4m 23s\tremaining: 1h 35m 23s\n",
      "44:\tlearn: 1.3773373\ttotal: 4m 29s\tremaining: 1h 35m 16s\n",
      "45:\tlearn: 1.3704958\ttotal: 4m 35s\tremaining: 1h 35m 9s\n",
      "46:\tlearn: 1.3577549\ttotal: 4m 41s\tremaining: 1h 35m 3s\n",
      "47:\tlearn: 1.3503187\ttotal: 4m 47s\tremaining: 1h 34m 58s\n",
      "48:\tlearn: 1.3402632\ttotal: 4m 53s\tremaining: 1h 34m 51s\n",
      "49:\tlearn: 1.3126129\ttotal: 4m 59s\tremaining: 1h 34m 43s\n",
      "50:\tlearn: 1.3073124\ttotal: 5m 5s\tremaining: 1h 34m 40s\n",
      "51:\tlearn: 1.3045692\ttotal: 5m 11s\tremaining: 1h 34m 35s\n",
      "52:\tlearn: 1.2881713\ttotal: 5m 17s\tremaining: 1h 34m 28s\n",
      "53:\tlearn: 1.2848256\ttotal: 5m 23s\tremaining: 1h 34m 24s\n",
      "54:\tlearn: 1.2776477\ttotal: 5m 29s\tremaining: 1h 34m 19s\n",
      "55:\tlearn: 1.2719388\ttotal: 5m 35s\tremaining: 1h 34m 14s\n",
      "56:\tlearn: 1.2604616\ttotal: 5m 41s\tremaining: 1h 34m 8s\n",
      "57:\tlearn: 1.2546000\ttotal: 5m 47s\tremaining: 1h 34m 2s\n",
      "58:\tlearn: 1.2388405\ttotal: 5m 53s\tremaining: 1h 33m 56s\n",
      "59:\tlearn: 1.2263519\ttotal: 5m 59s\tremaining: 1h 33m 49s\n",
      "60:\tlearn: 1.2256697\ttotal: 6m 5s\tremaining: 1h 33m 45s\n",
      "61:\tlearn: 1.2172336\ttotal: 6m 11s\tremaining: 1h 33m 39s\n",
      "62:\tlearn: 1.2021889\ttotal: 6m 17s\tremaining: 1h 33m 32s\n",
      "63:\tlearn: 1.1997598\ttotal: 6m 23s\tremaining: 1h 33m 28s\n",
      "64:\tlearn: 1.1945482\ttotal: 6m 29s\tremaining: 1h 33m 22s\n",
      "65:\tlearn: 1.1885409\ttotal: 6m 35s\tremaining: 1h 33m 16s\n",
      "66:\tlearn: 1.1698699\ttotal: 6m 41s\tremaining: 1h 33m 8s\n",
      "67:\tlearn: 1.1608781\ttotal: 6m 47s\tremaining: 1h 33m 2s\n",
      "68:\tlearn: 1.1518239\ttotal: 6m 53s\tremaining: 1h 32m 56s\n",
      "69:\tlearn: 1.1378813\ttotal: 6m 59s\tremaining: 1h 32m 49s\n",
      "70:\tlearn: 1.1278198\ttotal: 7m 5s\tremaining: 1h 32m 43s\n",
      "71:\tlearn: 1.1143694\ttotal: 7m 11s\tremaining: 1h 32m 37s\n",
      "72:\tlearn: 1.1037968\ttotal: 7m 17s\tremaining: 1h 32m 30s\n",
      "73:\tlearn: 1.1003564\ttotal: 7m 23s\tremaining: 1h 32m 24s\n",
      "74:\tlearn: 1.0933326\ttotal: 7m 29s\tremaining: 1h 32m 19s\n",
      "75:\tlearn: 1.0770119\ttotal: 7m 35s\tremaining: 1h 32m 12s\n",
      "76:\tlearn: 1.0663428\ttotal: 7m 41s\tremaining: 1h 32m 6s\n",
      "77:\tlearn: 1.0642097\ttotal: 7m 47s\tremaining: 1h 32m 1s\n",
      "78:\tlearn: 1.0549277\ttotal: 7m 53s\tremaining: 1h 31m 55s\n",
      "79:\tlearn: 1.0481154\ttotal: 7m 59s\tremaining: 1h 31m 49s\n",
      "80:\tlearn: 1.0401136\ttotal: 8m 5s\tremaining: 1h 31m 42s\n",
      "81:\tlearn: 1.0321982\ttotal: 8m 10s\tremaining: 1h 31m 36s\n",
      "82:\tlearn: 1.0278266\ttotal: 8m 16s\tremaining: 1h 31m 30s\n",
      "83:\tlearn: 1.0208002\ttotal: 8m 22s\tremaining: 1h 31m 24s\n",
      "84:\tlearn: 1.0187679\ttotal: 8m 29s\tremaining: 1h 31m 19s\n",
      "85:\tlearn: 1.0128193\ttotal: 8m 34s\tremaining: 1h 31m 13s\n",
      "86:\tlearn: 1.0032929\ttotal: 8m 40s\tremaining: 1h 31m 6s\n",
      "87:\tlearn: 0.9999501\ttotal: 8m 46s\tremaining: 1h 31m\n",
      "88:\tlearn: 0.9934345\ttotal: 8m 52s\tremaining: 1h 30m 54s\n",
      "89:\tlearn: 0.9918809\ttotal: 8m 58s\tremaining: 1h 30m 48s\n",
      "90:\tlearn: 0.9868315\ttotal: 9m 4s\tremaining: 1h 30m 42s\n",
      "91:\tlearn: 0.9773013\ttotal: 9m 10s\tremaining: 1h 30m 35s\n",
      "92:\tlearn: 0.9714838\ttotal: 9m 16s\tremaining: 1h 30m 29s\n",
      "93:\tlearn: 0.9631906\ttotal: 9m 22s\tremaining: 1h 30m 23s\n",
      "94:\tlearn: 0.9566249\ttotal: 9m 28s\tremaining: 1h 30m 17s\n",
      "95:\tlearn: 0.9543186\ttotal: 9m 34s\tremaining: 1h 30m 11s\n",
      "96:\tlearn: 0.9501124\ttotal: 9m 40s\tremaining: 1h 30m 6s\n",
      "97:\tlearn: 0.9454863\ttotal: 9m 46s\tremaining: 1h 29m 59s\n",
      "98:\tlearn: 0.9422311\ttotal: 9m 52s\tremaining: 1h 29m 53s\n",
      "99:\tlearn: 0.9375190\ttotal: 9m 58s\tremaining: 1h 29m 47s\n",
      "100:\tlearn: 0.9323507\ttotal: 10m 4s\tremaining: 1h 29m 41s\n",
      "101:\tlearn: 0.9292847\ttotal: 10m 10s\tremaining: 1h 29m 35s\n",
      "102:\tlearn: 0.9216574\ttotal: 10m 16s\tremaining: 1h 29m 29s\n",
      "103:\tlearn: 0.9206467\ttotal: 10m 22s\tremaining: 1h 29m 24s\n",
      "104:\tlearn: 0.9168894\ttotal: 10m 28s\tremaining: 1h 29m 18s\n",
      "105:\tlearn: 0.9099785\ttotal: 10m 34s\tremaining: 1h 29m 12s\n",
      "106:\tlearn: 0.9055899\ttotal: 10m 40s\tremaining: 1h 29m 6s\n",
      "107:\tlearn: 0.9009687\ttotal: 10m 46s\tremaining: 1h 29m 1s\n",
      "108:\tlearn: 0.8892855\ttotal: 10m 52s\tremaining: 1h 28m 54s\n",
      "109:\tlearn: 0.8868018\ttotal: 10m 58s\tremaining: 1h 28m 48s\n",
      "110:\tlearn: 0.8851366\ttotal: 11m 4s\tremaining: 1h 28m 43s\n",
      "111:\tlearn: 0.8818571\ttotal: 11m 10s\tremaining: 1h 28m 38s\n",
      "112:\tlearn: 0.8770250\ttotal: 11m 16s\tremaining: 1h 28m 32s\n",
      "113:\tlearn: 0.8733820\ttotal: 11m 22s\tremaining: 1h 28m 27s\n",
      "114:\tlearn: 0.8705213\ttotal: 11m 28s\tremaining: 1h 28m 21s\n",
      "115:\tlearn: 0.8640797\ttotal: 11m 34s\tremaining: 1h 28m 15s\n",
      "116:\tlearn: 0.8554105\ttotal: 11m 40s\tremaining: 1h 28m 8s\n",
      "117:\tlearn: 0.8519542\ttotal: 11m 46s\tremaining: 1h 28m 2s\n",
      "118:\tlearn: 0.8484602\ttotal: 11m 52s\tremaining: 1h 27m 57s\n",
      "119:\tlearn: 0.8456569\ttotal: 11m 58s\tremaining: 1h 27m 50s\n",
      "120:\tlearn: 0.8422948\ttotal: 12m 4s\tremaining: 1h 27m 45s\n",
      "121:\tlearn: 0.8397647\ttotal: 12m 10s\tremaining: 1h 27m 39s\n",
      "122:\tlearn: 0.8353263\ttotal: 12m 16s\tremaining: 1h 27m 33s\n",
      "123:\tlearn: 0.8332471\ttotal: 12m 22s\tremaining: 1h 27m 28s\n",
      "124:\tlearn: 0.8305564\ttotal: 12m 28s\tremaining: 1h 27m 22s\n",
      "125:\tlearn: 0.8281962\ttotal: 12m 35s\tremaining: 1h 27m 17s\n",
      "126:\tlearn: 0.8221446\ttotal: 12m 41s\tremaining: 1h 27m 11s\n",
      "127:\tlearn: 0.8215448\ttotal: 12m 47s\tremaining: 1h 27m 6s\n",
      "128:\tlearn: 0.8182921\ttotal: 12m 53s\tremaining: 1h 27m\n",
      "129:\tlearn: 0.8125056\ttotal: 12m 59s\tremaining: 1h 26m 54s\n",
      "130:\tlearn: 0.8091533\ttotal: 13m 5s\tremaining: 1h 26m 48s\n",
      "131:\tlearn: 0.8056187\ttotal: 13m 11s\tremaining: 1h 26m 42s\n",
      "132:\tlearn: 0.8003562\ttotal: 13m 17s\tremaining: 1h 26m 36s\n",
      "133:\tlearn: 0.7982654\ttotal: 13m 23s\tremaining: 1h 26m 30s\n",
      "134:\tlearn: 0.7966235\ttotal: 13m 29s\tremaining: 1h 26m 25s\n",
      "135:\tlearn: 0.7914629\ttotal: 13m 35s\tremaining: 1h 26m 18s\n",
      "136:\tlearn: 0.7897493\ttotal: 13m 41s\tremaining: 1h 26m 13s\n",
      "137:\tlearn: 0.7849696\ttotal: 13m 47s\tremaining: 1h 26m 6s\n",
      "138:\tlearn: 0.7844939\ttotal: 13m 53s\tremaining: 1h 26m 1s\n",
      "139:\tlearn: 0.7837631\ttotal: 13m 59s\tremaining: 1h 25m 56s\n",
      "140:\tlearn: 0.7824236\ttotal: 14m 5s\tremaining: 1h 25m 50s\n",
      "141:\tlearn: 0.7810861\ttotal: 14m 11s\tremaining: 1h 25m 45s\n",
      "142:\tlearn: 0.7784491\ttotal: 14m 17s\tremaining: 1h 25m 39s\n",
      "143:\tlearn: 0.7781822\ttotal: 14m 23s\tremaining: 1h 25m 34s\n",
      "144:\tlearn: 0.7754824\ttotal: 14m 29s\tremaining: 1h 25m 28s\n",
      "145:\tlearn: 0.7730728\ttotal: 14m 35s\tremaining: 1h 25m 22s\n",
      "146:\tlearn: 0.7707924\ttotal: 14m 41s\tremaining: 1h 25m 17s\n",
      "147:\tlearn: 0.7676847\ttotal: 14m 47s\tremaining: 1h 25m 11s\n",
      "148:\tlearn: 0.7659961\ttotal: 14m 54s\tremaining: 1h 25m 6s\n",
      "149:\tlearn: 0.7644979\ttotal: 15m\tremaining: 1h 25m\n",
      "150:\tlearn: 0.7596240\ttotal: 15m 6s\tremaining: 1h 24m 54s\n",
      "151:\tlearn: 0.7591742\ttotal: 15m 12s\tremaining: 1h 24m 49s\n",
      "152:\tlearn: 0.7568334\ttotal: 15m 18s\tremaining: 1h 24m 43s\n",
      "153:\tlearn: 0.7550106\ttotal: 15m 24s\tremaining: 1h 24m 37s\n",
      "154:\tlearn: 0.7526275\ttotal: 15m 30s\tremaining: 1h 24m 31s\n",
      "155:\tlearn: 0.7512898\ttotal: 15m 36s\tremaining: 1h 24m 26s\n",
      "156:\tlearn: 0.7498643\ttotal: 15m 42s\tremaining: 1h 24m 20s\n",
      "157:\tlearn: 0.7461878\ttotal: 15m 48s\tremaining: 1h 24m 14s\n",
      "158:\tlearn: 0.7450959\ttotal: 15m 54s\tremaining: 1h 24m 8s\n",
      "159:\tlearn: 0.7436221\ttotal: 16m 1s\tremaining: 1h 24m 5s\n",
      "160:\tlearn: 0.7415330\ttotal: 16m 7s\tremaining: 1h 23m 59s\n",
      "161:\tlearn: 0.7401049\ttotal: 16m 13s\tremaining: 1h 23m 53s\n",
      "162:\tlearn: 0.7350835\ttotal: 16m 19s\tremaining: 1h 23m 47s\n",
      "163:\tlearn: 0.7299328\ttotal: 16m 25s\tremaining: 1h 23m 41s\n",
      "164:\tlearn: 0.7281979\ttotal: 16m 30s\tremaining: 1h 23m 34s\n",
      "165:\tlearn: 0.7269281\ttotal: 16m 37s\tremaining: 1h 23m 29s\n",
      "166:\tlearn: 0.7258711\ttotal: 16m 43s\tremaining: 1h 23m 23s\n",
      "167:\tlearn: 0.7217963\ttotal: 16m 49s\tremaining: 1h 23m 17s\n",
      "168:\tlearn: 0.7195671\ttotal: 16m 54s\tremaining: 1h 23m 10s\n",
      "169:\tlearn: 0.7154976\ttotal: 17m\tremaining: 1h 23m 4s\n",
      "170:\tlearn: 0.7140203\ttotal: 17m 6s\tremaining: 1h 22m 58s\n",
      "171:\tlearn: 0.7137607\ttotal: 17m 13s\tremaining: 1h 22m 53s\n",
      "172:\tlearn: 0.7085523\ttotal: 17m 19s\tremaining: 1h 22m 46s\n",
      "173:\tlearn: 0.7063423\ttotal: 17m 24s\tremaining: 1h 22m 40s\n",
      "174:\tlearn: 0.7034697\ttotal: 17m 30s\tremaining: 1h 22m 34s\n",
      "175:\tlearn: 0.7029584\ttotal: 17m 37s\tremaining: 1h 22m 28s\n",
      "176:\tlearn: 0.7000190\ttotal: 17m 43s\tremaining: 1h 22m 22s\n",
      "177:\tlearn: 0.6990584\ttotal: 17m 49s\tremaining: 1h 22m 16s\n",
      "178:\tlearn: 0.6986500\ttotal: 17m 55s\tremaining: 1h 22m 11s\n",
      "179:\tlearn: 0.6974175\ttotal: 18m 1s\tremaining: 1h 22m 5s\n",
      "180:\tlearn: 0.6959649\ttotal: 18m 7s\tremaining: 1h 21m 59s\n",
      "181:\tlearn: 0.6930262\ttotal: 18m 13s\tremaining: 1h 21m 52s\n",
      "182:\tlearn: 0.6902780\ttotal: 18m 19s\tremaining: 1h 21m 46s\n",
      "183:\tlearn: 0.6893569\ttotal: 18m 25s\tremaining: 1h 21m 41s\n",
      "184:\tlearn: 0.6884128\ttotal: 18m 31s\tremaining: 1h 21m 35s\n",
      "185:\tlearn: 0.6843325\ttotal: 18m 37s\tremaining: 1h 21m 29s\n",
      "186:\tlearn: 0.6840154\ttotal: 18m 43s\tremaining: 1h 21m 23s\n",
      "187:\tlearn: 0.6826466\ttotal: 18m 49s\tremaining: 1h 21m 18s\n",
      "188:\tlearn: 0.6782190\ttotal: 18m 55s\tremaining: 1h 21m 11s\n",
      "189:\tlearn: 0.6754572\ttotal: 19m 1s\tremaining: 1h 21m 5s\n",
      "190:\tlearn: 0.6734935\ttotal: 19m 7s\tremaining: 1h 20m 59s\n",
      "191:\tlearn: 0.6723163\ttotal: 19m 13s\tremaining: 1h 20m 53s\n",
      "192:\tlearn: 0.6711118\ttotal: 19m 19s\tremaining: 1h 20m 47s\n",
      "193:\tlearn: 0.6672549\ttotal: 19m 25s\tremaining: 1h 20m 41s\n",
      "194:\tlearn: 0.6667482\ttotal: 19m 31s\tremaining: 1h 20m 35s\n",
      "195:\tlearn: 0.6664782\ttotal: 19m 37s\tremaining: 1h 20m 29s\n",
      "196:\tlearn: 0.6654144\ttotal: 19m 43s\tremaining: 1h 20m 23s\n",
      "197:\tlearn: 0.6648437\ttotal: 19m 49s\tremaining: 1h 20m 18s\n",
      "198:\tlearn: 0.6643800\ttotal: 19m 55s\tremaining: 1h 20m 12s\n",
      "199:\tlearn: 0.6643088\ttotal: 20m 1s\tremaining: 1h 20m 7s\n",
      "200:\tlearn: 0.6628442\ttotal: 20m 7s\tremaining: 1h 20m 1s\n",
      "201:\tlearn: 0.6619772\ttotal: 20m 13s\tremaining: 1h 19m 55s\n",
      "202:\tlearn: 0.6579070\ttotal: 20m 19s\tremaining: 1h 19m 49s\n",
      "203:\tlearn: 0.6565326\ttotal: 20m 25s\tremaining: 1h 19m 43s\n",
      "204:\tlearn: 0.6562449\ttotal: 20m 32s\tremaining: 1h 19m 38s\n",
      "205:\tlearn: 0.6549137\ttotal: 20m 38s\tremaining: 1h 19m 34s\n",
      "206:\tlearn: 0.6526894\ttotal: 20m 44s\tremaining: 1h 19m 29s\n",
      "207:\tlearn: 0.6508494\ttotal: 20m 50s\tremaining: 1h 19m 22s\n",
      "208:\tlearn: 0.6496526\ttotal: 20m 56s\tremaining: 1h 19m 16s\n",
      "209:\tlearn: 0.6491081\ttotal: 21m 3s\tremaining: 1h 19m 11s\n",
      "210:\tlearn: 0.6446482\ttotal: 21m 9s\tremaining: 1h 19m 5s\n",
      "211:\tlearn: 0.6442087\ttotal: 21m 15s\tremaining: 1h 18m 59s\n",
      "212:\tlearn: 0.6438516\ttotal: 21m 21s\tremaining: 1h 18m 53s\n",
      "213:\tlearn: 0.6401903\ttotal: 21m 27s\tremaining: 1h 18m 47s\n",
      "214:\tlearn: 0.6392901\ttotal: 21m 33s\tremaining: 1h 18m 41s\n",
      "215:\tlearn: 0.6380712\ttotal: 21m 39s\tremaining: 1h 18m 35s\n",
      "216:\tlearn: 0.6341853\ttotal: 21m 45s\tremaining: 1h 18m 29s\n",
      "217:\tlearn: 0.6336337\ttotal: 21m 51s\tremaining: 1h 18m 23s\n",
      "218:\tlearn: 0.6310754\ttotal: 21m 57s\tremaining: 1h 18m 17s\n",
      "219:\tlearn: 0.6292155\ttotal: 22m 3s\tremaining: 1h 18m 11s\n",
      "220:\tlearn: 0.6252570\ttotal: 22m 9s\tremaining: 1h 18m 4s\n",
      "221:\tlearn: 0.6246754\ttotal: 22m 15s\tremaining: 1h 17m 59s\n",
      "222:\tlearn: 0.6239924\ttotal: 22m 21s\tremaining: 1h 17m 53s\n",
      "223:\tlearn: 0.6230236\ttotal: 22m 27s\tremaining: 1h 17m 47s\n",
      "224:\tlearn: 0.6229054\ttotal: 22m 33s\tremaining: 1h 17m 41s\n",
      "225:\tlearn: 0.6222548\ttotal: 22m 39s\tremaining: 1h 17m 35s\n",
      "226:\tlearn: 0.6198452\ttotal: 22m 45s\tremaining: 1h 17m 29s\n",
      "227:\tlearn: 0.6190233\ttotal: 22m 51s\tremaining: 1h 17m 23s\n",
      "228:\tlearn: 0.6174508\ttotal: 22m 57s\tremaining: 1h 17m 18s\n",
      "229:\tlearn: 0.6164578\ttotal: 23m 3s\tremaining: 1h 17m 12s\n",
      "230:\tlearn: 0.6162318\ttotal: 23m 9s\tremaining: 1h 17m 6s\n",
      "231:\tlearn: 0.6151135\ttotal: 23m 15s\tremaining: 1h 17m\n",
      "232:\tlearn: 0.6140877\ttotal: 23m 21s\tremaining: 1h 16m 55s\n",
      "233:\tlearn: 0.6128949\ttotal: 23m 27s\tremaining: 1h 16m 48s\n",
      "234:\tlearn: 0.6119839\ttotal: 23m 33s\tremaining: 1h 16m 42s\n",
      "235:\tlearn: 0.6107260\ttotal: 23m 39s\tremaining: 1h 16m 36s\n",
      "236:\tlearn: 0.6102244\ttotal: 23m 45s\tremaining: 1h 16m 30s\n",
      "237:\tlearn: 0.6099407\ttotal: 23m 52s\tremaining: 1h 16m 24s\n",
      "238:\tlearn: 0.6085195\ttotal: 23m 58s\tremaining: 1h 16m 18s\n",
      "239:\tlearn: 0.6080196\ttotal: 24m 4s\tremaining: 1h 16m 13s\n",
      "240:\tlearn: 0.6079958\ttotal: 24m 10s\tremaining: 1h 16m 7s\n",
      "241:\tlearn: 0.6073186\ttotal: 24m 16s\tremaining: 1h 16m 1s\n",
      "242:\tlearn: 0.6064320\ttotal: 24m 22s\tremaining: 1h 15m 56s\n",
      "243:\tlearn: 0.6061849\ttotal: 24m 28s\tremaining: 1h 15m 50s\n",
      "244:\tlearn: 0.6061281\ttotal: 24m 34s\tremaining: 1h 15m 44s\n",
      "245:\tlearn: 0.6022017\ttotal: 24m 40s\tremaining: 1h 15m 38s\n",
      "246:\tlearn: 0.6021039\ttotal: 24m 46s\tremaining: 1h 15m 32s\n",
      "247:\tlearn: 0.6004656\ttotal: 24m 52s\tremaining: 1h 15m 26s\n",
      "248:\tlearn: 0.5983136\ttotal: 24m 58s\tremaining: 1h 15m 20s\n",
      "249:\tlearn: 0.5980027\ttotal: 25m 4s\tremaining: 1h 15m 14s\n",
      "250:\tlearn: 0.5973311\ttotal: 25m 10s\tremaining: 1h 15m 8s\n",
      "251:\tlearn: 0.5972424\ttotal: 25m 16s\tremaining: 1h 15m 2s\n",
      "252:\tlearn: 0.5961920\ttotal: 25m 23s\tremaining: 1h 14m 56s\n",
      "253:\tlearn: 0.5958040\ttotal: 25m 29s\tremaining: 1h 14m 50s\n",
      "254:\tlearn: 0.5936765\ttotal: 25m 35s\tremaining: 1h 14m 44s\n",
      "255:\tlearn: 0.5922172\ttotal: 25m 41s\tremaining: 1h 14m 38s\n",
      "256:\tlearn: 0.5910874\ttotal: 25m 47s\tremaining: 1h 14m 32s\n",
      "257:\tlearn: 0.5900171\ttotal: 25m 53s\tremaining: 1h 14m 26s\n",
      "258:\tlearn: 0.5881041\ttotal: 25m 59s\tremaining: 1h 14m 20s\n",
      "259:\tlearn: 0.5878572\ttotal: 26m 5s\tremaining: 1h 14m 14s\n",
      "260:\tlearn: 0.5858856\ttotal: 26m 11s\tremaining: 1h 14m 8s\n",
      "261:\tlearn: 0.5849521\ttotal: 26m 17s\tremaining: 1h 14m 2s\n",
      "262:\tlearn: 0.5847638\ttotal: 26m 23s\tremaining: 1h 13m 57s\n",
      "263:\tlearn: 0.5845147\ttotal: 26m 29s\tremaining: 1h 13m 51s\n",
      "264:\tlearn: 0.5832174\ttotal: 26m 35s\tremaining: 1h 13m 45s\n",
      "265:\tlearn: 0.5816232\ttotal: 26m 41s\tremaining: 1h 13m 39s\n",
      "266:\tlearn: 0.5804171\ttotal: 26m 47s\tremaining: 1h 13m 33s\n",
      "267:\tlearn: 0.5799842\ttotal: 26m 53s\tremaining: 1h 13m 27s\n",
      "268:\tlearn: 0.5787201\ttotal: 26m 59s\tremaining: 1h 13m 20s\n",
      "269:\tlearn: 0.5786438\ttotal: 27m 5s\tremaining: 1h 13m 15s\n",
      "270:\tlearn: 0.5768714\ttotal: 27m 11s\tremaining: 1h 13m 9s\n",
      "271:\tlearn: 0.5763696\ttotal: 27m 17s\tremaining: 1h 13m 3s\n",
      "272:\tlearn: 0.5743521\ttotal: 27m 23s\tremaining: 1h 12m 57s\n",
      "273:\tlearn: 0.5724671\ttotal: 27m 29s\tremaining: 1h 12m 50s\n",
      "274:\tlearn: 0.5696565\ttotal: 27m 35s\tremaining: 1h 12m 44s\n",
      "275:\tlearn: 0.5694115\ttotal: 27m 41s\tremaining: 1h 12m 38s\n",
      "276:\tlearn: 0.5685213\ttotal: 27m 47s\tremaining: 1h 12m 32s\n",
      "277:\tlearn: 0.5682991\ttotal: 27m 53s\tremaining: 1h 12m 27s\n",
      "278:\tlearn: 0.5679681\ttotal: 27m 59s\tremaining: 1h 12m 21s\n",
      "279:\tlearn: 0.5665904\ttotal: 28m 5s\tremaining: 1h 12m 15s\n",
      "280:\tlearn: 0.5650914\ttotal: 28m 11s\tremaining: 1h 12m 9s\n",
      "281:\tlearn: 0.5647926\ttotal: 28m 17s\tremaining: 1h 12m 3s\n",
      "282:\tlearn: 0.5634911\ttotal: 28m 24s\tremaining: 1h 11m 57s\n",
      "283:\tlearn: 0.5609350\ttotal: 28m 29s\tremaining: 1h 11m 51s\n",
      "284:\tlearn: 0.5587708\ttotal: 28m 35s\tremaining: 1h 11m 44s\n",
      "285:\tlearn: 0.5582866\ttotal: 28m 41s\tremaining: 1h 11m 38s\n",
      "286:\tlearn: 0.5548103\ttotal: 28m 47s\tremaining: 1h 11m 32s\n",
      "287:\tlearn: 0.5546019\ttotal: 28m 53s\tremaining: 1h 11m 26s\n",
      "288:\tlearn: 0.5533812\ttotal: 29m\tremaining: 1h 11m 20s\n",
      "289:\tlearn: 0.5533455\ttotal: 29m 6s\tremaining: 1h 11m 15s\n",
      "290:\tlearn: 0.5524944\ttotal: 29m 12s\tremaining: 1h 11m 9s\n",
      "291:\tlearn: 0.5522691\ttotal: 29m 18s\tremaining: 1h 11m 3s\n",
      "292:\tlearn: 0.5513637\ttotal: 29m 24s\tremaining: 1h 10m 57s\n",
      "293:\tlearn: 0.5493901\ttotal: 29m 30s\tremaining: 1h 10m 51s\n",
      "294:\tlearn: 0.5477801\ttotal: 29m 36s\tremaining: 1h 10m 45s\n",
      "295:\tlearn: 0.5462701\ttotal: 29m 42s\tremaining: 1h 10m 39s\n",
      "296:\tlearn: 0.5452708\ttotal: 29m 48s\tremaining: 1h 10m 33s\n",
      "297:\tlearn: 0.5450910\ttotal: 29m 54s\tremaining: 1h 10m 27s\n",
      "298:\tlearn: 0.5440897\ttotal: 30m\tremaining: 1h 10m 21s\n",
      "299:\tlearn: 0.5431916\ttotal: 30m 6s\tremaining: 1h 10m 15s\n",
      "300:\tlearn: 0.5424836\ttotal: 30m 12s\tremaining: 1h 10m 9s\n",
      "301:\tlearn: 0.5409407\ttotal: 30m 18s\tremaining: 1h 10m 3s\n",
      "302:\tlearn: 0.5401231\ttotal: 30m 24s\tremaining: 1h 9m 57s\n",
      "303:\tlearn: 0.5387144\ttotal: 30m 30s\tremaining: 1h 9m 51s\n",
      "304:\tlearn: 0.5384606\ttotal: 30m 36s\tremaining: 1h 9m 45s\n",
      "305:\tlearn: 0.5384422\ttotal: 30m 42s\tremaining: 1h 9m 39s\n",
      "306:\tlearn: 0.5376074\ttotal: 30m 48s\tremaining: 1h 9m 33s\n",
      "307:\tlearn: 0.5375448\ttotal: 30m 54s\tremaining: 1h 9m 27s\n",
      "308:\tlearn: 0.5353056\ttotal: 31m\tremaining: 1h 9m 21s\n",
      "309:\tlearn: 0.5346154\ttotal: 31m 6s\tremaining: 1h 9m 15s\n",
      "310:\tlearn: 0.5345900\ttotal: 31m 13s\tremaining: 1h 9m 10s\n",
      "311:\tlearn: 0.5334863\ttotal: 31m 19s\tremaining: 1h 9m 4s\n",
      "312:\tlearn: 0.5321794\ttotal: 31m 25s\tremaining: 1h 8m 58s\n",
      "313:\tlearn: 0.5296752\ttotal: 31m 31s\tremaining: 1h 8m 52s\n",
      "314:\tlearn: 0.5279795\ttotal: 31m 37s\tremaining: 1h 8m 46s\n",
      "315:\tlearn: 0.5279576\ttotal: 31m 43s\tremaining: 1h 8m 40s\n",
      "316:\tlearn: 0.5279307\ttotal: 31m 49s\tremaining: 1h 8m 34s\n",
      "317:\tlearn: 0.5274533\ttotal: 31m 55s\tremaining: 1h 8m 28s\n",
      "318:\tlearn: 0.5268016\ttotal: 32m 1s\tremaining: 1h 8m 22s\n",
      "319:\tlearn: 0.5247655\ttotal: 32m 7s\tremaining: 1h 8m 16s\n",
      "320:\tlearn: 0.5239034\ttotal: 32m 13s\tremaining: 1h 8m 10s\n",
      "321:\tlearn: 0.5231332\ttotal: 32m 19s\tremaining: 1h 8m 4s\n",
      "322:\tlearn: 0.5218429\ttotal: 32m 25s\tremaining: 1h 7m 58s\n",
      "323:\tlearn: 0.5205512\ttotal: 32m 31s\tremaining: 1h 7m 52s\n",
      "324:\tlearn: 0.5204203\ttotal: 32m 38s\tremaining: 1h 7m 46s\n",
      "325:\tlearn: 0.5197284\ttotal: 32m 44s\tremaining: 1h 7m 40s\n",
      "326:\tlearn: 0.5192680\ttotal: 32m 49s\tremaining: 1h 7m 34s\n",
      "327:\tlearn: 0.5188996\ttotal: 32m 56s\tremaining: 1h 7m 28s\n",
      "328:\tlearn: 0.5177854\ttotal: 33m 1s\tremaining: 1h 7m 22s\n",
      "329:\tlearn: 0.5172664\ttotal: 33m 8s\tremaining: 1h 7m 16s\n",
      "330:\tlearn: 0.5164564\ttotal: 33m 14s\tremaining: 1h 7m 10s\n",
      "331:\tlearn: 0.5162296\ttotal: 33m 20s\tremaining: 1h 7m 4s\n",
      "332:\tlearn: 0.5154997\ttotal: 33m 26s\tremaining: 1h 6m 58s\n",
      "333:\tlearn: 0.5151143\ttotal: 33m 32s\tremaining: 1h 6m 52s\n",
      "334:\tlearn: 0.5146038\ttotal: 33m 38s\tremaining: 1h 6m 46s\n",
      "335:\tlearn: 0.5122053\ttotal: 33m 44s\tremaining: 1h 6m 39s\n",
      "336:\tlearn: 0.5120470\ttotal: 33m 50s\tremaining: 1h 6m 34s\n",
      "337:\tlearn: 0.5104854\ttotal: 33m 56s\tremaining: 1h 6m 27s\n",
      "338:\tlearn: 0.5097538\ttotal: 34m 2s\tremaining: 1h 6m 21s\n",
      "339:\tlearn: 0.5093673\ttotal: 34m 8s\tremaining: 1h 6m 16s\n",
      "340:\tlearn: 0.5087720\ttotal: 34m 14s\tremaining: 1h 6m 10s\n",
      "341:\tlearn: 0.5073903\ttotal: 34m 20s\tremaining: 1h 6m 3s\n",
      "342:\tlearn: 0.5067039\ttotal: 34m 26s\tremaining: 1h 5m 57s\n",
      "343:\tlearn: 0.5049911\ttotal: 34m 32s\tremaining: 1h 5m 51s\n",
      "344:\tlearn: 0.5045522\ttotal: 34m 38s\tremaining: 1h 5m 45s\n",
      "345:\tlearn: 0.5035131\ttotal: 34m 44s\tremaining: 1h 5m 39s\n",
      "346:\tlearn: 0.5026651\ttotal: 34m 50s\tremaining: 1h 5m 33s\n",
      "347:\tlearn: 0.5021372\ttotal: 34m 56s\tremaining: 1h 5m 27s\n",
      "348:\tlearn: 0.5014316\ttotal: 35m 2s\tremaining: 1h 5m 21s\n",
      "349:\tlearn: 0.4998900\ttotal: 35m 8s\tremaining: 1h 5m 15s\n",
      "350:\tlearn: 0.4982409\ttotal: 35m 14s\tremaining: 1h 5m 8s\n",
      "351:\tlearn: 0.4973114\ttotal: 35m 20s\tremaining: 1h 5m 2s\n",
      "352:\tlearn: 0.4961000\ttotal: 35m 26s\tremaining: 1h 4m 56s\n",
      "353:\tlearn: 0.4954248\ttotal: 35m 32s\tremaining: 1h 4m 50s\n",
      "354:\tlearn: 0.4943896\ttotal: 35m 38s\tremaining: 1h 4m 44s\n",
      "355:\tlearn: 0.4921079\ttotal: 35m 43s\tremaining: 1h 4m 38s\n",
      "356:\tlearn: 0.4919467\ttotal: 35m 50s\tremaining: 1h 4m 32s\n",
      "357:\tlearn: 0.4906846\ttotal: 35m 55s\tremaining: 1h 4m 26s\n",
      "358:\tlearn: 0.4905161\ttotal: 36m 2s\tremaining: 1h 4m 20s\n",
      "359:\tlearn: 0.4890476\ttotal: 36m 7s\tremaining: 1h 4m 14s\n",
      "360:\tlearn: 0.4888077\ttotal: 36m 14s\tremaining: 1h 4m 8s\n",
      "361:\tlearn: 0.4879724\ttotal: 36m 20s\tremaining: 1h 4m 2s\n",
      "362:\tlearn: 0.4869168\ttotal: 36m 26s\tremaining: 1h 3m 56s\n",
      "363:\tlearn: 0.4847155\ttotal: 36m 31s\tremaining: 1h 3m 49s\n",
      "364:\tlearn: 0.4839792\ttotal: 36m 37s\tremaining: 1h 3m 43s\n",
      "365:\tlearn: 0.4839112\ttotal: 36m 44s\tremaining: 1h 3m 37s\n",
      "366:\tlearn: 0.4834800\ttotal: 36m 50s\tremaining: 1h 3m 32s\n",
      "367:\tlearn: 0.4824283\ttotal: 36m 56s\tremaining: 1h 3m 25s\n",
      "368:\tlearn: 0.4814256\ttotal: 37m 1s\tremaining: 1h 3m 19s\n",
      "369:\tlearn: 0.4800866\ttotal: 37m 7s\tremaining: 1h 3m 13s\n",
      "370:\tlearn: 0.4789395\ttotal: 37m 13s\tremaining: 1h 3m 7s\n",
      "371:\tlearn: 0.4783948\ttotal: 37m 19s\tremaining: 1h 3m 1s\n",
      "372:\tlearn: 0.4769733\ttotal: 37m 25s\tremaining: 1h 2m 55s\n",
      "373:\tlearn: 0.4769520\ttotal: 37m 31s\tremaining: 1h 2m 49s\n",
      "374:\tlearn: 0.4761511\ttotal: 37m 37s\tremaining: 1h 2m 43s\n",
      "375:\tlearn: 0.4761348\ttotal: 37m 43s\tremaining: 1h 2m 37s\n",
      "376:\tlearn: 0.4750460\ttotal: 37m 49s\tremaining: 1h 2m 31s\n",
      "377:\tlearn: 0.4746317\ttotal: 37m 55s\tremaining: 1h 2m 25s\n",
      "378:\tlearn: 0.4744181\ttotal: 38m 2s\tremaining: 1h 2m 19s\n",
      "379:\tlearn: 0.4739303\ttotal: 38m 7s\tremaining: 1h 2m 13s\n",
      "380:\tlearn: 0.4737787\ttotal: 38m 14s\tremaining: 1h 2m 7s\n",
      "381:\tlearn: 0.4723274\ttotal: 38m 20s\tremaining: 1h 2m\n",
      "382:\tlearn: 0.4721711\ttotal: 38m 26s\tremaining: 1h 1m 55s\n",
      "383:\tlearn: 0.4721586\ttotal: 38m 32s\tremaining: 1h 1m 49s\n",
      "384:\tlearn: 0.4711388\ttotal: 38m 38s\tremaining: 1h 1m 43s\n",
      "385:\tlearn: 0.4695457\ttotal: 38m 44s\tremaining: 1h 1m 37s\n",
      "386:\tlearn: 0.4685044\ttotal: 38m 50s\tremaining: 1h 1m 31s\n",
      "387:\tlearn: 0.4675489\ttotal: 38m 56s\tremaining: 1h 1m 25s\n",
      "388:\tlearn: 0.4669715\ttotal: 39m 2s\tremaining: 1h 1m 19s\n",
      "389:\tlearn: 0.4657820\ttotal: 39m 8s\tremaining: 1h 1m 12s\n",
      "390:\tlearn: 0.4655923\ttotal: 39m 14s\tremaining: 1h 1m 6s\n",
      "391:\tlearn: 0.4649132\ttotal: 39m 20s\tremaining: 1h 1m\n",
      "392:\tlearn: 0.4639552\ttotal: 39m 26s\tremaining: 1h 54s\n",
      "393:\tlearn: 0.4635889\ttotal: 39m 32s\tremaining: 1h 48s\n",
      "394:\tlearn: 0.4623487\ttotal: 39m 38s\tremaining: 1h 42s\n",
      "395:\tlearn: 0.4620508\ttotal: 39m 44s\tremaining: 1h 36s\n",
      "396:\tlearn: 0.4619529\ttotal: 39m 50s\tremaining: 1h 30s\n",
      "397:\tlearn: 0.4617635\ttotal: 39m 56s\tremaining: 1h 24s\n",
      "398:\tlearn: 0.4616870\ttotal: 40m 2s\tremaining: 1h 18s\n",
      "399:\tlearn: 0.4605210\ttotal: 40m 8s\tremaining: 1h 12s\n",
      "400:\tlearn: 0.4601564\ttotal: 40m 14s\tremaining: 1h 6s\n",
      "401:\tlearn: 0.4595933\ttotal: 40m 20s\tremaining: 1h\n",
      "402:\tlearn: 0.4595862\ttotal: 40m 26s\tremaining: 59m 54s\n",
      "403:\tlearn: 0.4591383\ttotal: 40m 32s\tremaining: 59m 48s\n",
      "404:\tlearn: 0.4590808\ttotal: 40m 38s\tremaining: 59m 42s\n",
      "405:\tlearn: 0.4580440\ttotal: 40m 44s\tremaining: 59m 36s\n",
      "406:\tlearn: 0.4578197\ttotal: 40m 50s\tremaining: 59m 30s\n",
      "407:\tlearn: 0.4561601\ttotal: 40m 56s\tremaining: 59m 24s\n",
      "408:\tlearn: 0.4547994\ttotal: 41m 2s\tremaining: 59m 18s\n",
      "409:\tlearn: 0.4533668\ttotal: 41m 8s\tremaining: 59m 12s\n",
      "410:\tlearn: 0.4522102\ttotal: 41m 14s\tremaining: 59m 6s\n",
      "411:\tlearn: 0.4521682\ttotal: 41m 20s\tremaining: 59m\n",
      "412:\tlearn: 0.4518963\ttotal: 41m 26s\tremaining: 58m 54s\n",
      "413:\tlearn: 0.4512710\ttotal: 41m 32s\tremaining: 58m 48s\n",
      "414:\tlearn: 0.4507000\ttotal: 41m 38s\tremaining: 58m 42s\n",
      "415:\tlearn: 0.4497609\ttotal: 41m 44s\tremaining: 58m 36s\n",
      "416:\tlearn: 0.4485407\ttotal: 41m 50s\tremaining: 58m 30s\n",
      "417:\tlearn: 0.4481878\ttotal: 41m 56s\tremaining: 58m 24s\n",
      "418:\tlearn: 0.4480697\ttotal: 42m 2s\tremaining: 58m 18s\n",
      "419:\tlearn: 0.4473292\ttotal: 42m 8s\tremaining: 58m 12s\n",
      "420:\tlearn: 0.4472506\ttotal: 42m 15s\tremaining: 58m 6s\n",
      "421:\tlearn: 0.4457886\ttotal: 42m 20s\tremaining: 58m\n",
      "422:\tlearn: 0.4456495\ttotal: 42m 27s\tremaining: 57m 54s\n",
      "423:\tlearn: 0.4445255\ttotal: 42m 33s\tremaining: 57m 48s\n",
      "424:\tlearn: 0.4435272\ttotal: 42m 38s\tremaining: 57m 42s\n",
      "425:\tlearn: 0.4428873\ttotal: 42m 44s\tremaining: 57m 36s\n",
      "426:\tlearn: 0.4424484\ttotal: 42m 50s\tremaining: 57m 30s\n",
      "427:\tlearn: 0.4420629\ttotal: 42m 56s\tremaining: 57m 23s\n",
      "428:\tlearn: 0.4419763\ttotal: 43m 2s\tremaining: 57m 17s\n",
      "429:\tlearn: 0.4413196\ttotal: 43m 8s\tremaining: 57m 11s\n",
      "430:\tlearn: 0.4409117\ttotal: 43m 14s\tremaining: 57m 5s\n",
      "431:\tlearn: 0.4400293\ttotal: 43m 20s\tremaining: 56m 59s\n",
      "432:\tlearn: 0.4396044\ttotal: 43m 26s\tremaining: 56m 53s\n",
      "433:\tlearn: 0.4374629\ttotal: 43m 32s\tremaining: 56m 47s\n",
      "434:\tlearn: 0.4373416\ttotal: 43m 38s\tremaining: 56m 41s\n",
      "435:\tlearn: 0.4372005\ttotal: 43m 45s\tremaining: 56m 35s\n",
      "436:\tlearn: 0.4367928\ttotal: 43m 51s\tremaining: 56m 29s\n",
      "437:\tlearn: 0.4361157\ttotal: 43m 57s\tremaining: 56m 23s\n",
      "438:\tlearn: 0.4359978\ttotal: 44m 3s\tremaining: 56m 17s\n",
      "439:\tlearn: 0.4354711\ttotal: 44m 9s\tremaining: 56m 11s\n",
      "440:\tlearn: 0.4333652\ttotal: 44m 15s\tremaining: 56m 5s\n",
      "441:\tlearn: 0.4333556\ttotal: 44m 21s\tremaining: 55m 59s\n",
      "442:\tlearn: 0.4330405\ttotal: 44m 27s\tremaining: 55m 53s\n",
      "443:\tlearn: 0.4319176\ttotal: 44m 32s\tremaining: 55m 47s\n",
      "444:\tlearn: 0.4312320\ttotal: 44m 38s\tremaining: 55m 41s\n",
      "445:\tlearn: 0.4310331\ttotal: 44m 44s\tremaining: 55m 35s\n",
      "446:\tlearn: 0.4308817\ttotal: 44m 51s\tremaining: 55m 29s\n",
      "447:\tlearn: 0.4307468\ttotal: 44m 57s\tremaining: 55m 23s\n",
      "448:\tlearn: 0.4302283\ttotal: 45m 3s\tremaining: 55m 17s\n",
      "449:\tlearn: 0.4300459\ttotal: 45m 8s\tremaining: 55m 10s\n",
      "450:\tlearn: 0.4297813\ttotal: 45m 15s\tremaining: 55m 5s\n",
      "451:\tlearn: 0.4288664\ttotal: 45m 21s\tremaining: 54m 59s\n",
      "452:\tlearn: 0.4284416\ttotal: 45m 27s\tremaining: 54m 53s\n",
      "453:\tlearn: 0.4282116\ttotal: 45m 33s\tremaining: 54m 47s\n",
      "454:\tlearn: 0.4274673\ttotal: 45m 39s\tremaining: 54m 41s\n",
      "455:\tlearn: 0.4262950\ttotal: 45m 45s\tremaining: 54m 34s\n",
      "456:\tlearn: 0.4256639\ttotal: 45m 51s\tremaining: 54m 28s\n",
      "457:\tlearn: 0.4245039\ttotal: 45m 57s\tremaining: 54m 22s\n",
      "458:\tlearn: 0.4243752\ttotal: 46m 3s\tremaining: 54m 16s\n",
      "459:\tlearn: 0.4238457\ttotal: 46m 9s\tremaining: 54m 10s\n",
      "460:\tlearn: 0.4237979\ttotal: 46m 15s\tremaining: 54m 5s\n",
      "461:\tlearn: 0.4235148\ttotal: 46m 21s\tremaining: 53m 59s\n",
      "462:\tlearn: 0.4235085\ttotal: 46m 28s\tremaining: 53m 54s\n",
      "463:\tlearn: 0.4222546\ttotal: 46m 34s\tremaining: 53m 48s\n",
      "464:\tlearn: 0.4216556\ttotal: 46m 40s\tremaining: 53m 41s\n",
      "465:\tlearn: 0.4199496\ttotal: 46m 46s\tremaining: 53m 35s\n",
      "466:\tlearn: 0.4188129\ttotal: 46m 52s\tremaining: 53m 29s\n",
      "467:\tlearn: 0.4184689\ttotal: 46m 58s\tremaining: 53m 23s\n",
      "468:\tlearn: 0.4181714\ttotal: 47m 4s\tremaining: 53m 17s\n",
      "469:\tlearn: 0.4172222\ttotal: 47m 10s\tremaining: 53m 11s\n",
      "470:\tlearn: 0.4165788\ttotal: 47m 16s\tremaining: 53m 5s\n",
      "471:\tlearn: 0.4161798\ttotal: 47m 22s\tremaining: 52m 59s\n",
      "472:\tlearn: 0.4159862\ttotal: 47m 28s\tremaining: 52m 53s\n",
      "473:\tlearn: 0.4154902\ttotal: 47m 34s\tremaining: 52m 47s\n",
      "474:\tlearn: 0.4151940\ttotal: 47m 40s\tremaining: 52m 41s\n",
      "475:\tlearn: 0.4142033\ttotal: 47m 46s\tremaining: 52m 35s\n",
      "476:\tlearn: 0.4140611\ttotal: 47m 52s\tremaining: 52m 29s\n",
      "477:\tlearn: 0.4139151\ttotal: 47m 58s\tremaining: 52m 23s\n",
      "478:\tlearn: 0.4135523\ttotal: 48m 4s\tremaining: 52m 17s\n",
      "479:\tlearn: 0.4128021\ttotal: 48m 10s\tremaining: 52m 10s\n",
      "480:\tlearn: 0.4116159\ttotal: 48m 15s\tremaining: 52m 4s\n",
      "481:\tlearn: 0.4115113\ttotal: 48m 22s\tremaining: 51m 58s\n",
      "482:\tlearn: 0.4106269\ttotal: 48m 28s\tremaining: 51m 52s\n",
      "483:\tlearn: 0.4093722\ttotal: 48m 33s\tremaining: 51m 46s\n",
      "484:\tlearn: 0.4084944\ttotal: 48m 39s\tremaining: 51m 40s\n",
      "485:\tlearn: 0.4071021\ttotal: 48m 45s\tremaining: 51m 34s\n",
      "486:\tlearn: 0.4070372\ttotal: 48m 52s\tremaining: 51m 28s\n",
      "487:\tlearn: 0.4068231\ttotal: 48m 58s\tremaining: 51m 22s\n",
      "488:\tlearn: 0.4067569\ttotal: 49m 4s\tremaining: 51m 16s\n",
      "489:\tlearn: 0.4052113\ttotal: 49m 10s\tremaining: 51m 10s\n",
      "490:\tlearn: 0.4043664\ttotal: 49m 16s\tremaining: 51m 4s\n",
      "491:\tlearn: 0.4040750\ttotal: 49m 22s\tremaining: 50m 58s\n",
      "492:\tlearn: 0.4034059\ttotal: 49m 28s\tremaining: 50m 52s\n",
      "493:\tlearn: 0.4031847\ttotal: 49m 34s\tremaining: 50m 46s\n",
      "494:\tlearn: 0.4028115\ttotal: 49m 40s\tremaining: 50m 40s\n",
      "495:\tlearn: 0.4023931\ttotal: 49m 46s\tremaining: 50m 34s\n",
      "496:\tlearn: 0.4018438\ttotal: 49m 52s\tremaining: 50m 28s\n",
      "497:\tlearn: 0.4016499\ttotal: 49m 58s\tremaining: 50m 22s\n",
      "498:\tlearn: 0.4006897\ttotal: 50m 4s\tremaining: 50m 16s\n",
      "499:\tlearn: 0.4001858\ttotal: 50m 10s\tremaining: 50m 10s\n",
      "500:\tlearn: 0.4000534\ttotal: 50m 16s\tremaining: 50m 4s\n",
      "501:\tlearn: 0.4000492\ttotal: 50m 22s\tremaining: 49m 58s\n",
      "502:\tlearn: 0.3997617\ttotal: 50m 28s\tremaining: 49m 52s\n",
      "503:\tlearn: 0.3987606\ttotal: 50m 34s\tremaining: 49m 46s\n",
      "504:\tlearn: 0.3977470\ttotal: 50m 40s\tremaining: 49m 40s\n",
      "505:\tlearn: 0.3974694\ttotal: 50m 46s\tremaining: 49m 34s\n",
      "506:\tlearn: 0.3971048\ttotal: 50m 52s\tremaining: 49m 28s\n",
      "507:\tlearn: 0.3970137\ttotal: 50m 58s\tremaining: 49m 22s\n",
      "508:\tlearn: 0.3967144\ttotal: 51m 4s\tremaining: 49m 16s\n",
      "509:\tlearn: 0.3960595\ttotal: 51m 10s\tremaining: 49m 10s\n",
      "510:\tlearn: 0.3954893\ttotal: 51m 16s\tremaining: 49m 4s\n",
      "511:\tlearn: 0.3947444\ttotal: 51m 22s\tremaining: 48m 58s\n",
      "512:\tlearn: 0.3943669\ttotal: 51m 28s\tremaining: 48m 51s\n",
      "513:\tlearn: 0.3939600\ttotal: 51m 34s\tremaining: 48m 45s\n",
      "514:\tlearn: 0.3930074\ttotal: 51m 40s\tremaining: 48m 39s\n",
      "515:\tlearn: 0.3925578\ttotal: 51m 46s\tremaining: 48m 33s\n",
      "516:\tlearn: 0.3911404\ttotal: 51m 52s\tremaining: 48m 27s\n",
      "517:\tlearn: 0.3907175\ttotal: 51m 58s\tremaining: 48m 21s\n",
      "518:\tlearn: 0.3905975\ttotal: 52m 4s\tremaining: 48m 15s\n",
      "519:\tlearn: 0.3899710\ttotal: 52m 10s\tremaining: 48m 9s\n",
      "520:\tlearn: 0.3898523\ttotal: 52m 16s\tremaining: 48m 3s\n",
      "521:\tlearn: 0.3898420\ttotal: 52m 22s\tremaining: 47m 57s\n",
      "522:\tlearn: 0.3895444\ttotal: 52m 28s\tremaining: 47m 51s\n",
      "523:\tlearn: 0.3892385\ttotal: 52m 34s\tremaining: 47m 45s\n",
      "524:\tlearn: 0.3889935\ttotal: 52m 40s\tremaining: 47m 39s\n",
      "525:\tlearn: 0.3883568\ttotal: 52m 46s\tremaining: 47m 33s\n",
      "526:\tlearn: 0.3874639\ttotal: 52m 52s\tremaining: 47m 27s\n",
      "527:\tlearn: 0.3865099\ttotal: 52m 58s\tremaining: 47m 21s\n",
      "528:\tlearn: 0.3861071\ttotal: 53m 4s\tremaining: 47m 15s\n",
      "529:\tlearn: 0.3850318\ttotal: 53m 10s\tremaining: 47m 9s\n",
      "530:\tlearn: 0.3848465\ttotal: 53m 16s\tremaining: 47m 3s\n",
      "531:\tlearn: 0.3839872\ttotal: 53m 22s\tremaining: 46m 57s\n",
      "532:\tlearn: 0.3837049\ttotal: 53m 28s\tremaining: 46m 51s\n",
      "533:\tlearn: 0.3830791\ttotal: 53m 34s\tremaining: 46m 45s\n",
      "534:\tlearn: 0.3828200\ttotal: 53m 40s\tremaining: 46m 39s\n",
      "535:\tlearn: 0.3827483\ttotal: 53m 46s\tremaining: 46m 33s\n",
      "536:\tlearn: 0.3825782\ttotal: 53m 52s\tremaining: 46m 27s\n",
      "537:\tlearn: 0.3817796\ttotal: 53m 58s\tremaining: 46m 21s\n",
      "538:\tlearn: 0.3815466\ttotal: 54m 4s\tremaining: 46m 15s\n",
      "539:\tlearn: 0.3809493\ttotal: 54m 10s\tremaining: 46m 9s\n",
      "540:\tlearn: 0.3808494\ttotal: 54m 16s\tremaining: 46m 3s\n",
      "541:\tlearn: 0.3800986\ttotal: 54m 22s\tremaining: 45m 57s\n",
      "542:\tlearn: 0.3800494\ttotal: 54m 28s\tremaining: 45m 51s\n",
      "543:\tlearn: 0.3788225\ttotal: 54m 34s\tremaining: 45m 45s\n",
      "544:\tlearn: 0.3783161\ttotal: 54m 40s\tremaining: 45m 39s\n",
      "545:\tlearn: 0.3783045\ttotal: 54m 46s\tremaining: 45m 33s\n",
      "546:\tlearn: 0.3775246\ttotal: 54m 52s\tremaining: 45m 27s\n",
      "547:\tlearn: 0.3768744\ttotal: 54m 58s\tremaining: 45m 20s\n",
      "548:\tlearn: 0.3754274\ttotal: 55m 4s\tremaining: 45m 14s\n",
      "549:\tlearn: 0.3749160\ttotal: 55m 10s\tremaining: 45m 8s\n",
      "550:\tlearn: 0.3745887\ttotal: 55m 16s\tremaining: 45m 2s\n",
      "551:\tlearn: 0.3743296\ttotal: 55m 22s\tremaining: 44m 56s\n",
      "552:\tlearn: 0.3741256\ttotal: 55m 28s\tremaining: 44m 50s\n",
      "553:\tlearn: 0.3740297\ttotal: 55m 34s\tremaining: 44m 44s\n",
      "554:\tlearn: 0.3728156\ttotal: 55m 40s\tremaining: 44m 38s\n",
      "555:\tlearn: 0.3726223\ttotal: 55m 46s\tremaining: 44m 32s\n",
      "556:\tlearn: 0.3725054\ttotal: 55m 52s\tremaining: 44m 26s\n",
      "557:\tlearn: 0.3722130\ttotal: 55m 58s\tremaining: 44m 20s\n",
      "558:\tlearn: 0.3715150\ttotal: 56m 4s\tremaining: 44m 14s\n",
      "559:\tlearn: 0.3709123\ttotal: 56m 10s\tremaining: 44m 8s\n",
      "560:\tlearn: 0.3706311\ttotal: 56m 16s\tremaining: 44m 2s\n",
      "561:\tlearn: 0.3698995\ttotal: 56m 22s\tremaining: 43m 56s\n",
      "562:\tlearn: 0.3694310\ttotal: 56m 28s\tremaining: 43m 50s\n",
      "563:\tlearn: 0.3685872\ttotal: 56m 34s\tremaining: 43m 44s\n",
      "564:\tlearn: 0.3680836\ttotal: 56m 40s\tremaining: 43m 38s\n",
      "565:\tlearn: 0.3667246\ttotal: 56m 46s\tremaining: 43m 32s\n",
      "566:\tlearn: 0.3661861\ttotal: 56m 52s\tremaining: 43m 26s\n",
      "567:\tlearn: 0.3660701\ttotal: 56m 58s\tremaining: 43m 20s\n",
      "568:\tlearn: 0.3655320\ttotal: 57m 4s\tremaining: 43m 14s\n",
      "569:\tlearn: 0.3651086\ttotal: 57m 10s\tremaining: 43m 8s\n",
      "570:\tlearn: 0.3650140\ttotal: 57m 16s\tremaining: 43m 2s\n",
      "571:\tlearn: 0.3645210\ttotal: 57m 22s\tremaining: 42m 56s\n",
      "572:\tlearn: 0.3645166\ttotal: 57m 28s\tremaining: 42m 50s\n",
      "573:\tlearn: 0.3638847\ttotal: 57m 34s\tremaining: 42m 44s\n",
      "574:\tlearn: 0.3635052\ttotal: 57m 40s\tremaining: 42m 38s\n",
      "575:\tlearn: 0.3634622\ttotal: 57m 46s\tremaining: 42m 32s\n",
      "576:\tlearn: 0.3629011\ttotal: 57m 52s\tremaining: 42m 26s\n",
      "577:\tlearn: 0.3624993\ttotal: 57m 58s\tremaining: 42m 19s\n",
      "578:\tlearn: 0.3623620\ttotal: 58m 5s\tremaining: 42m 14s\n",
      "579:\tlearn: 0.3612508\ttotal: 58m 10s\tremaining: 42m 7s\n",
      "580:\tlearn: 0.3602314\ttotal: 58m 16s\tremaining: 42m 1s\n",
      "581:\tlearn: 0.3598558\ttotal: 58m 22s\tremaining: 41m 55s\n",
      "582:\tlearn: 0.3596387\ttotal: 58m 28s\tremaining: 41m 49s\n",
      "583:\tlearn: 0.3589417\ttotal: 58m 34s\tremaining: 41m 43s\n",
      "584:\tlearn: 0.3588462\ttotal: 58m 40s\tremaining: 41m 37s\n",
      "585:\tlearn: 0.3582873\ttotal: 58m 46s\tremaining: 41m 31s\n",
      "586:\tlearn: 0.3577494\ttotal: 58m 52s\tremaining: 41m 25s\n",
      "587:\tlearn: 0.3576360\ttotal: 58m 58s\tremaining: 41m 19s\n",
      "588:\tlearn: 0.3573260\ttotal: 59m 4s\tremaining: 41m 13s\n",
      "589:\tlearn: 0.3568534\ttotal: 59m 10s\tremaining: 41m 7s\n",
      "590:\tlearn: 0.3563676\ttotal: 59m 16s\tremaining: 41m 1s\n",
      "591:\tlearn: 0.3555053\ttotal: 59m 22s\tremaining: 40m 55s\n",
      "592:\tlearn: 0.3549548\ttotal: 59m 28s\tremaining: 40m 49s\n",
      "593:\tlearn: 0.3541566\ttotal: 59m 34s\tremaining: 40m 43s\n",
      "594:\tlearn: 0.3539843\ttotal: 59m 40s\tremaining: 40m 37s\n",
      "595:\tlearn: 0.3532159\ttotal: 59m 46s\tremaining: 40m 31s\n",
      "596:\tlearn: 0.3524572\ttotal: 59m 52s\tremaining: 40m 24s\n",
      "597:\tlearn: 0.3522779\ttotal: 59m 58s\tremaining: 40m 18s\n",
      "598:\tlearn: 0.3518995\ttotal: 1h 4s\tremaining: 40m 12s\n",
      "599:\tlearn: 0.3511821\ttotal: 1h 10s\tremaining: 40m 6s\n",
      "600:\tlearn: 0.3510894\ttotal: 1h 16s\tremaining: 40m\n",
      "601:\tlearn: 0.3507975\ttotal: 1h 22s\tremaining: 39m 54s\n",
      "602:\tlearn: 0.3506757\ttotal: 1h 28s\tremaining: 39m 48s\n",
      "603:\tlearn: 0.3502204\ttotal: 1h 34s\tremaining: 39m 42s\n",
      "604:\tlearn: 0.3493694\ttotal: 1h 40s\tremaining: 39m 36s\n",
      "605:\tlearn: 0.3490694\ttotal: 1h 46s\tremaining: 39m 30s\n",
      "606:\tlearn: 0.3486456\ttotal: 1h 52s\tremaining: 39m 24s\n",
      "607:\tlearn: 0.3479411\ttotal: 1h 58s\tremaining: 39m 18s\n",
      "608:\tlearn: 0.3478595\ttotal: 1h 1m 4s\tremaining: 39m 12s\n",
      "609:\tlearn: 0.3476278\ttotal: 1h 1m 10s\tremaining: 39m 6s\n",
      "610:\tlearn: 0.3473599\ttotal: 1h 1m 16s\tremaining: 39m\n",
      "611:\tlearn: 0.3466056\ttotal: 1h 1m 22s\tremaining: 38m 54s\n",
      "612:\tlearn: 0.3464789\ttotal: 1h 1m 28s\tremaining: 38m 48s\n",
      "613:\tlearn: 0.3462358\ttotal: 1h 1m 34s\tremaining: 38m 42s\n",
      "614:\tlearn: 0.3461567\ttotal: 1h 1m 40s\tremaining: 38m 36s\n",
      "615:\tlearn: 0.3458439\ttotal: 1h 1m 46s\tremaining: 38m 30s\n",
      "616:\tlearn: 0.3457632\ttotal: 1h 1m 52s\tremaining: 38m 24s\n",
      "617:\tlearn: 0.3451411\ttotal: 1h 1m 58s\tremaining: 38m 18s\n",
      "618:\tlearn: 0.3447829\ttotal: 1h 2m 4s\tremaining: 38m 12s\n",
      "619:\tlearn: 0.3446073\ttotal: 1h 2m 10s\tremaining: 38m 6s\n",
      "620:\tlearn: 0.3433247\ttotal: 1h 2m 16s\tremaining: 38m\n",
      "621:\tlearn: 0.3428511\ttotal: 1h 2m 22s\tremaining: 37m 54s\n",
      "622:\tlearn: 0.3426755\ttotal: 1h 2m 28s\tremaining: 37m 48s\n",
      "623:\tlearn: 0.3426330\ttotal: 1h 2m 34s\tremaining: 37m 42s\n",
      "624:\tlearn: 0.3415657\ttotal: 1h 2m 40s\tremaining: 37m 36s\n",
      "625:\tlearn: 0.3410400\ttotal: 1h 2m 46s\tremaining: 37m 30s\n",
      "626:\tlearn: 0.3407755\ttotal: 1h 2m 52s\tremaining: 37m 24s\n",
      "627:\tlearn: 0.3407703\ttotal: 1h 2m 58s\tremaining: 37m 18s\n",
      "628:\tlearn: 0.3406349\ttotal: 1h 3m 4s\tremaining: 37m 12s\n",
      "629:\tlearn: 0.3401992\ttotal: 1h 3m 10s\tremaining: 37m 6s\n",
      "630:\tlearn: 0.3399932\ttotal: 1h 3m 16s\tremaining: 37m\n",
      "631:\tlearn: 0.3395951\ttotal: 1h 3m 22s\tremaining: 36m 54s\n",
      "632:\tlearn: 0.3393923\ttotal: 1h 3m 28s\tremaining: 36m 48s\n",
      "633:\tlearn: 0.3391230\ttotal: 1h 3m 34s\tremaining: 36m 42s\n",
      "634:\tlearn: 0.3385295\ttotal: 1h 3m 40s\tremaining: 36m 36s\n",
      "635:\tlearn: 0.3382882\ttotal: 1h 3m 46s\tremaining: 36m 29s\n",
      "636:\tlearn: 0.3382589\ttotal: 1h 3m 52s\tremaining: 36m 23s\n",
      "637:\tlearn: 0.3376962\ttotal: 1h 3m 58s\tremaining: 36m 17s\n",
      "638:\tlearn: 0.3373300\ttotal: 1h 4m 4s\tremaining: 36m 11s\n",
      "639:\tlearn: 0.3372496\ttotal: 1h 4m 10s\tremaining: 36m 5s\n",
      "640:\tlearn: 0.3370591\ttotal: 1h 4m 16s\tremaining: 35m 59s\n",
      "641:\tlearn: 0.3369874\ttotal: 1h 4m 22s\tremaining: 35m 53s\n",
      "642:\tlearn: 0.3368061\ttotal: 1h 4m 28s\tremaining: 35m 47s\n",
      "643:\tlearn: 0.3365839\ttotal: 1h 4m 34s\tremaining: 35m 42s\n",
      "644:\tlearn: 0.3358094\ttotal: 1h 4m 40s\tremaining: 35m 35s\n",
      "645:\tlearn: 0.3355827\ttotal: 1h 4m 46s\tremaining: 35m 29s\n",
      "646:\tlearn: 0.3345560\ttotal: 1h 4m 52s\tremaining: 35m 23s\n",
      "647:\tlearn: 0.3332262\ttotal: 1h 4m 58s\tremaining: 35m 17s\n",
      "648:\tlearn: 0.3327636\ttotal: 1h 5m 4s\tremaining: 35m 11s\n",
      "649:\tlearn: 0.3316367\ttotal: 1h 5m 10s\tremaining: 35m 5s\n",
      "650:\tlearn: 0.3315966\ttotal: 1h 5m 16s\tremaining: 34m 59s\n",
      "651:\tlearn: 0.3314555\ttotal: 1h 5m 22s\tremaining: 34m 53s\n",
      "652:\tlearn: 0.3312650\ttotal: 1h 5m 28s\tremaining: 34m 47s\n",
      "653:\tlearn: 0.3305518\ttotal: 1h 5m 34s\tremaining: 34m 41s\n",
      "654:\tlearn: 0.3304637\ttotal: 1h 5m 40s\tremaining: 34m 35s\n",
      "655:\tlearn: 0.3303963\ttotal: 1h 5m 46s\tremaining: 34m 29s\n",
      "656:\tlearn: 0.3302626\ttotal: 1h 5m 52s\tremaining: 34m 23s\n",
      "657:\tlearn: 0.3302155\ttotal: 1h 5m 58s\tremaining: 34m 17s\n",
      "658:\tlearn: 0.3297394\ttotal: 1h 6m 4s\tremaining: 34m 11s\n",
      "659:\tlearn: 0.3294299\ttotal: 1h 6m 10s\tremaining: 34m 5s\n",
      "660:\tlearn: 0.3292724\ttotal: 1h 6m 17s\tremaining: 33m 59s\n",
      "661:\tlearn: 0.3292127\ttotal: 1h 6m 23s\tremaining: 33m 53s\n",
      "662:\tlearn: 0.3291590\ttotal: 1h 6m 29s\tremaining: 33m 47s\n",
      "663:\tlearn: 0.3288818\ttotal: 1h 6m 35s\tremaining: 33m 41s\n",
      "664:\tlearn: 0.3286619\ttotal: 1h 6m 41s\tremaining: 33m 35s\n",
      "665:\tlearn: 0.3285315\ttotal: 1h 6m 47s\tremaining: 33m 29s\n",
      "666:\tlearn: 0.3281197\ttotal: 1h 6m 53s\tremaining: 33m 23s\n",
      "667:\tlearn: 0.3275095\ttotal: 1h 6m 59s\tremaining: 33m 17s\n",
      "668:\tlearn: 0.3272885\ttotal: 1h 7m 5s\tremaining: 33m 11s\n",
      "669:\tlearn: 0.3272298\ttotal: 1h 7m 11s\tremaining: 33m 5s\n",
      "670:\tlearn: 0.3266071\ttotal: 1h 7m 17s\tremaining: 32m 59s\n",
      "671:\tlearn: 0.3259794\ttotal: 1h 7m 23s\tremaining: 32m 53s\n",
      "672:\tlearn: 0.3257811\ttotal: 1h 7m 29s\tremaining: 32m 47s\n",
      "673:\tlearn: 0.3257208\ttotal: 1h 7m 35s\tremaining: 32m 41s\n",
      "674:\tlearn: 0.3254408\ttotal: 1h 7m 41s\tremaining: 32m 35s\n",
      "675:\tlearn: 0.3248105\ttotal: 1h 7m 47s\tremaining: 32m 29s\n",
      "676:\tlearn: 0.3246776\ttotal: 1h 7m 53s\tremaining: 32m 23s\n",
      "677:\tlearn: 0.3241767\ttotal: 1h 7m 59s\tremaining: 32m 17s\n",
      "678:\tlearn: 0.3234359\ttotal: 1h 8m 5s\tremaining: 32m 11s\n",
      "679:\tlearn: 0.3232060\ttotal: 1h 8m 11s\tremaining: 32m 5s\n",
      "680:\tlearn: 0.3231911\ttotal: 1h 8m 17s\tremaining: 31m 59s\n",
      "681:\tlearn: 0.3229425\ttotal: 1h 8m 23s\tremaining: 31m 53s\n",
      "682:\tlearn: 0.3228732\ttotal: 1h 8m 29s\tremaining: 31m 47s\n",
      "683:\tlearn: 0.3228367\ttotal: 1h 8m 35s\tremaining: 31m 41s\n",
      "684:\tlearn: 0.3222084\ttotal: 1h 8m 41s\tremaining: 31m 35s\n",
      "685:\tlearn: 0.3219350\ttotal: 1h 8m 47s\tremaining: 31m 29s\n",
      "686:\tlearn: 0.3214876\ttotal: 1h 8m 53s\tremaining: 31m 23s\n",
      "687:\tlearn: 0.3212783\ttotal: 1h 8m 59s\tremaining: 31m 17s\n",
      "688:\tlearn: 0.3207581\ttotal: 1h 9m 5s\tremaining: 31m 11s\n",
      "689:\tlearn: 0.3206633\ttotal: 1h 9m 11s\tremaining: 31m 5s\n",
      "690:\tlearn: 0.3206345\ttotal: 1h 9m 17s\tremaining: 30m 59s\n",
      "691:\tlearn: 0.3205406\ttotal: 1h 9m 23s\tremaining: 30m 53s\n",
      "692:\tlearn: 0.3200389\ttotal: 1h 9m 29s\tremaining: 30m 47s\n",
      "693:\tlearn: 0.3194299\ttotal: 1h 9m 35s\tremaining: 30m 41s\n",
      "694:\tlearn: 0.3192030\ttotal: 1h 9m 41s\tremaining: 30m 35s\n",
      "695:\tlearn: 0.3189234\ttotal: 1h 9m 47s\tremaining: 30m 29s\n",
      "696:\tlearn: 0.3186114\ttotal: 1h 9m 53s\tremaining: 30m 23s\n",
      "697:\tlearn: 0.3184954\ttotal: 1h 9m 59s\tremaining: 30m 16s\n",
      "698:\tlearn: 0.3184928\ttotal: 1h 10m 5s\tremaining: 30m 11s\n",
      "699:\tlearn: 0.3179466\ttotal: 1h 10m 11s\tremaining: 30m 5s\n",
      "700:\tlearn: 0.3170644\ttotal: 1h 10m 17s\tremaining: 29m 58s\n",
      "701:\tlearn: 0.3166661\ttotal: 1h 10m 23s\tremaining: 29m 52s\n",
      "702:\tlearn: 0.3163847\ttotal: 1h 10m 29s\tremaining: 29m 46s\n",
      "703:\tlearn: 0.3161697\ttotal: 1h 10m 35s\tremaining: 29m 40s\n",
      "704:\tlearn: 0.3159765\ttotal: 1h 10m 41s\tremaining: 29m 34s\n",
      "705:\tlearn: 0.3159696\ttotal: 1h 10m 47s\tremaining: 29m 28s\n",
      "706:\tlearn: 0.3157979\ttotal: 1h 10m 53s\tremaining: 29m 22s\n",
      "707:\tlearn: 0.3151074\ttotal: 1h 10m 59s\tremaining: 29m 16s\n",
      "708:\tlearn: 0.3147539\ttotal: 1h 11m 5s\tremaining: 29m 10s\n",
      "709:\tlearn: 0.3147111\ttotal: 1h 11m 11s\tremaining: 29m 4s\n",
      "710:\tlearn: 0.3145068\ttotal: 1h 11m 17s\tremaining: 28m 58s\n",
      "711:\tlearn: 0.3138756\ttotal: 1h 11m 23s\tremaining: 28m 52s\n",
      "712:\tlearn: 0.3138109\ttotal: 1h 11m 29s\tremaining: 28m 46s\n",
      "713:\tlearn: 0.3136550\ttotal: 1h 11m 35s\tremaining: 28m 40s\n",
      "714:\tlearn: 0.3130341\ttotal: 1h 11m 41s\tremaining: 28m 34s\n",
      "715:\tlearn: 0.3122178\ttotal: 1h 11m 47s\tremaining: 28m 28s\n",
      "716:\tlearn: 0.3112412\ttotal: 1h 11m 53s\tremaining: 28m 22s\n",
      "717:\tlearn: 0.3112391\ttotal: 1h 11m 59s\tremaining: 28m 16s\n",
      "718:\tlearn: 0.3111107\ttotal: 1h 12m 5s\tremaining: 28m 10s\n",
      "719:\tlearn: 0.3109642\ttotal: 1h 12m 11s\tremaining: 28m 4s\n",
      "720:\tlearn: 0.3103157\ttotal: 1h 12m 17s\tremaining: 27m 58s\n",
      "721:\tlearn: 0.3101569\ttotal: 1h 12m 23s\tremaining: 27m 52s\n",
      "722:\tlearn: 0.3098351\ttotal: 1h 12m 29s\tremaining: 27m 46s\n",
      "723:\tlearn: 0.3089765\ttotal: 1h 12m 35s\tremaining: 27m 40s\n",
      "724:\tlearn: 0.3088236\ttotal: 1h 12m 41s\tremaining: 27m 34s\n",
      "725:\tlearn: 0.3084950\ttotal: 1h 12m 47s\tremaining: 27m 28s\n",
      "726:\tlearn: 0.3079739\ttotal: 1h 12m 53s\tremaining: 27m 22s\n",
      "727:\tlearn: 0.3076792\ttotal: 1h 12m 59s\tremaining: 27m 16s\n",
      "728:\tlearn: 0.3076605\ttotal: 1h 13m 5s\tremaining: 27m 10s\n",
      "729:\tlearn: 0.3074728\ttotal: 1h 13m 11s\tremaining: 27m 4s\n",
      "730:\tlearn: 0.3073452\ttotal: 1h 13m 17s\tremaining: 26m 58s\n",
      "731:\tlearn: 0.3072081\ttotal: 1h 13m 23s\tremaining: 26m 52s\n",
      "732:\tlearn: 0.3070236\ttotal: 1h 13m 29s\tremaining: 26m 46s\n",
      "733:\tlearn: 0.3066363\ttotal: 1h 13m 35s\tremaining: 26m 40s\n",
      "734:\tlearn: 0.3062297\ttotal: 1h 13m 41s\tremaining: 26m 34s\n",
      "735:\tlearn: 0.3061811\ttotal: 1h 13m 47s\tremaining: 26m 28s\n",
      "736:\tlearn: 0.3054478\ttotal: 1h 13m 53s\tremaining: 26m 22s\n",
      "737:\tlearn: 0.3050482\ttotal: 1h 13m 59s\tremaining: 26m 15s\n",
      "738:\tlearn: 0.3046543\ttotal: 1h 14m 5s\tremaining: 26m 9s\n",
      "739:\tlearn: 0.3044368\ttotal: 1h 14m 11s\tremaining: 26m 3s\n",
      "740:\tlearn: 0.3042718\ttotal: 1h 14m 17s\tremaining: 25m 57s\n",
      "741:\tlearn: 0.3041303\ttotal: 1h 14m 23s\tremaining: 25m 51s\n",
      "742:\tlearn: 0.3040620\ttotal: 1h 14m 29s\tremaining: 25m 45s\n",
      "743:\tlearn: 0.3040023\ttotal: 1h 14m 35s\tremaining: 25m 39s\n",
      "744:\tlearn: 0.3032396\ttotal: 1h 14m 41s\tremaining: 25m 33s\n",
      "745:\tlearn: 0.3032337\ttotal: 1h 14m 47s\tremaining: 25m 27s\n",
      "746:\tlearn: 0.3030350\ttotal: 1h 14m 53s\tremaining: 25m 21s\n",
      "747:\tlearn: 0.3020294\ttotal: 1h 14m 59s\tremaining: 25m 15s\n",
      "748:\tlearn: 0.3016865\ttotal: 1h 15m 5s\tremaining: 25m 9s\n",
      "749:\tlearn: 0.3012298\ttotal: 1h 15m 11s\tremaining: 25m 3s\n",
      "750:\tlearn: 0.3009698\ttotal: 1h 15m 17s\tremaining: 24m 57s\n",
      "751:\tlearn: 0.3009174\ttotal: 1h 15m 23s\tremaining: 24m 51s\n",
      "752:\tlearn: 0.3003930\ttotal: 1h 15m 29s\tremaining: 24m 45s\n",
      "753:\tlearn: 0.3003319\ttotal: 1h 15m 35s\tremaining: 24m 39s\n",
      "754:\tlearn: 0.3000906\ttotal: 1h 15m 41s\tremaining: 24m 33s\n",
      "755:\tlearn: 0.2998005\ttotal: 1h 15m 47s\tremaining: 24m 27s\n",
      "756:\tlearn: 0.2995522\ttotal: 1h 15m 53s\tremaining: 24m 21s\n",
      "757:\tlearn: 0.2989525\ttotal: 1h 15m 59s\tremaining: 24m 15s\n",
      "758:\tlearn: 0.2987265\ttotal: 1h 16m 5s\tremaining: 24m 9s\n",
      "759:\tlearn: 0.2987232\ttotal: 1h 16m 11s\tremaining: 24m 3s\n",
      "760:\tlearn: 0.2984738\ttotal: 1h 16m 17s\tremaining: 23m 57s\n",
      "761:\tlearn: 0.2982871\ttotal: 1h 16m 23s\tremaining: 23m 51s\n",
      "762:\tlearn: 0.2977712\ttotal: 1h 16m 29s\tremaining: 23m 45s\n",
      "763:\tlearn: 0.2975937\ttotal: 1h 16m 35s\tremaining: 23m 39s\n",
      "764:\tlearn: 0.2972814\ttotal: 1h 16m 41s\tremaining: 23m 33s\n",
      "765:\tlearn: 0.2970895\ttotal: 1h 16m 47s\tremaining: 23m 27s\n",
      "766:\tlearn: 0.2968566\ttotal: 1h 16m 53s\tremaining: 23m 21s\n",
      "767:\tlearn: 0.2963231\ttotal: 1h 16m 59s\tremaining: 23m 15s\n",
      "768:\tlearn: 0.2960426\ttotal: 1h 17m 5s\tremaining: 23m 9s\n",
      "769:\tlearn: 0.2960112\ttotal: 1h 17m 11s\tremaining: 23m 3s\n",
      "770:\tlearn: 0.2959695\ttotal: 1h 17m 17s\tremaining: 22m 57s\n",
      "771:\tlearn: 0.2957776\ttotal: 1h 17m 23s\tremaining: 22m 51s\n",
      "772:\tlearn: 0.2955625\ttotal: 1h 17m 29s\tremaining: 22m 45s\n",
      "773:\tlearn: 0.2948236\ttotal: 1h 17m 35s\tremaining: 22m 39s\n",
      "774:\tlearn: 0.2945881\ttotal: 1h 17m 41s\tremaining: 22m 33s\n",
      "775:\tlearn: 0.2944453\ttotal: 1h 17m 47s\tremaining: 22m 27s\n",
      "776:\tlearn: 0.2943741\ttotal: 1h 17m 53s\tremaining: 22m 21s\n",
      "777:\tlearn: 0.2939831\ttotal: 1h 17m 59s\tremaining: 22m 15s\n",
      "778:\tlearn: 0.2935413\ttotal: 1h 18m 5s\tremaining: 22m 9s\n",
      "779:\tlearn: 0.2934960\ttotal: 1h 18m 11s\tremaining: 22m 3s\n",
      "780:\tlearn: 0.2933460\ttotal: 1h 18m 17s\tremaining: 21m 57s\n",
      "781:\tlearn: 0.2930519\ttotal: 1h 18m 23s\tremaining: 21m 51s\n",
      "782:\tlearn: 0.2928210\ttotal: 1h 18m 29s\tremaining: 21m 45s\n",
      "783:\tlearn: 0.2926692\ttotal: 1h 18m 35s\tremaining: 21m 39s\n",
      "784:\tlearn: 0.2926250\ttotal: 1h 18m 41s\tremaining: 21m 33s\n",
      "785:\tlearn: 0.2924944\ttotal: 1h 18m 47s\tremaining: 21m 27s\n",
      "786:\tlearn: 0.2921219\ttotal: 1h 18m 53s\tremaining: 21m 21s\n",
      "787:\tlearn: 0.2917466\ttotal: 1h 18m 59s\tremaining: 21m 15s\n",
      "788:\tlearn: 0.2917142\ttotal: 1h 19m 5s\tremaining: 21m 9s\n",
      "789:\tlearn: 0.2915077\ttotal: 1h 19m 11s\tremaining: 21m 3s\n",
      "790:\tlearn: 0.2910200\ttotal: 1h 19m 17s\tremaining: 20m 57s\n",
      "791:\tlearn: 0.2906468\ttotal: 1h 19m 23s\tremaining: 20m 51s\n",
      "792:\tlearn: 0.2901661\ttotal: 1h 19m 29s\tremaining: 20m 45s\n",
      "793:\tlearn: 0.2898143\ttotal: 1h 19m 35s\tremaining: 20m 39s\n",
      "794:\tlearn: 0.2896591\ttotal: 1h 19m 41s\tremaining: 20m 32s\n",
      "795:\tlearn: 0.2895284\ttotal: 1h 19m 47s\tremaining: 20m 26s\n",
      "796:\tlearn: 0.2895260\ttotal: 1h 19m 53s\tremaining: 20m 21s\n",
      "797:\tlearn: 0.2892922\ttotal: 1h 19m 59s\tremaining: 20m 14s\n",
      "798:\tlearn: 0.2887654\ttotal: 1h 20m 5s\tremaining: 20m 8s\n",
      "799:\tlearn: 0.2882707\ttotal: 1h 20m 11s\tremaining: 20m 2s\n",
      "800:\tlearn: 0.2876898\ttotal: 1h 20m 17s\tremaining: 19m 56s\n",
      "801:\tlearn: 0.2876098\ttotal: 1h 20m 23s\tremaining: 19m 50s\n",
      "802:\tlearn: 0.2872021\ttotal: 1h 20m 29s\tremaining: 19m 44s\n",
      "803:\tlearn: 0.2868916\ttotal: 1h 20m 35s\tremaining: 19m 38s\n",
      "804:\tlearn: 0.2866697\ttotal: 1h 20m 41s\tremaining: 19m 32s\n",
      "805:\tlearn: 0.2861540\ttotal: 1h 20m 47s\tremaining: 19m 26s\n",
      "806:\tlearn: 0.2859277\ttotal: 1h 20m 53s\tremaining: 19m 20s\n",
      "807:\tlearn: 0.2856117\ttotal: 1h 20m 59s\tremaining: 19m 14s\n",
      "808:\tlearn: 0.2850569\ttotal: 1h 21m 5s\tremaining: 19m 8s\n",
      "809:\tlearn: 0.2845658\ttotal: 1h 21m 11s\tremaining: 19m 2s\n",
      "810:\tlearn: 0.2842114\ttotal: 1h 21m 17s\tremaining: 18m 56s\n",
      "811:\tlearn: 0.2841393\ttotal: 1h 21m 23s\tremaining: 18m 50s\n",
      "812:\tlearn: 0.2840012\ttotal: 1h 21m 29s\tremaining: 18m 44s\n",
      "813:\tlearn: 0.2836248\ttotal: 1h 21m 35s\tremaining: 18m 38s\n",
      "814:\tlearn: 0.2831892\ttotal: 1h 21m 41s\tremaining: 18m 32s\n",
      "815:\tlearn: 0.2829681\ttotal: 1h 21m 47s\tremaining: 18m 26s\n",
      "816:\tlearn: 0.2828546\ttotal: 1h 21m 53s\tremaining: 18m 20s\n",
      "817:\tlearn: 0.2825409\ttotal: 1h 21m 59s\tremaining: 18m 14s\n",
      "818:\tlearn: 0.2823469\ttotal: 1h 22m 5s\tremaining: 18m 8s\n",
      "819:\tlearn: 0.2818429\ttotal: 1h 22m 11s\tremaining: 18m 2s\n",
      "820:\tlearn: 0.2816431\ttotal: 1h 22m 17s\tremaining: 17m 56s\n",
      "821:\tlearn: 0.2814259\ttotal: 1h 22m 23s\tremaining: 17m 50s\n",
      "822:\tlearn: 0.2812971\ttotal: 1h 22m 28s\tremaining: 17m 44s\n",
      "823:\tlearn: 0.2810821\ttotal: 1h 22m 34s\tremaining: 17m 38s\n",
      "824:\tlearn: 0.2806851\ttotal: 1h 22m 40s\tremaining: 17m 32s\n",
      "825:\tlearn: 0.2806339\ttotal: 1h 22m 46s\tremaining: 17m 26s\n",
      "826:\tlearn: 0.2804813\ttotal: 1h 22m 52s\tremaining: 17m 20s\n",
      "827:\tlearn: 0.2801383\ttotal: 1h 22m 58s\tremaining: 17m 14s\n",
      "828:\tlearn: 0.2799579\ttotal: 1h 23m 4s\tremaining: 17m 8s\n",
      "829:\tlearn: 0.2794999\ttotal: 1h 23m 10s\tremaining: 17m 2s\n",
      "830:\tlearn: 0.2792828\ttotal: 1h 23m 16s\tremaining: 16m 56s\n",
      "831:\tlearn: 0.2791675\ttotal: 1h 23m 22s\tremaining: 16m 50s\n",
      "832:\tlearn: 0.2789216\ttotal: 1h 23m 28s\tremaining: 16m 44s\n",
      "833:\tlearn: 0.2788193\ttotal: 1h 23m 34s\tremaining: 16m 38s\n",
      "834:\tlearn: 0.2787757\ttotal: 1h 23m 40s\tremaining: 16m 32s\n",
      "835:\tlearn: 0.2787290\ttotal: 1h 23m 46s\tremaining: 16m 26s\n",
      "836:\tlearn: 0.2783681\ttotal: 1h 23m 52s\tremaining: 16m 20s\n",
      "837:\tlearn: 0.2782228\ttotal: 1h 23m 58s\tremaining: 16m 14s\n",
      "838:\tlearn: 0.2780832\ttotal: 1h 24m 4s\tremaining: 16m 8s\n",
      "839:\tlearn: 0.2780000\ttotal: 1h 24m 10s\tremaining: 16m 2s\n",
      "840:\tlearn: 0.2775628\ttotal: 1h 24m 16s\tremaining: 15m 56s\n",
      "841:\tlearn: 0.2774159\ttotal: 1h 24m 22s\tremaining: 15m 50s\n",
      "842:\tlearn: 0.2770057\ttotal: 1h 24m 28s\tremaining: 15m 43s\n",
      "843:\tlearn: 0.2761817\ttotal: 1h 24m 34s\tremaining: 15m 37s\n",
      "844:\tlearn: 0.2757863\ttotal: 1h 24m 40s\tremaining: 15m 31s\n",
      "845:\tlearn: 0.2756139\ttotal: 1h 24m 46s\tremaining: 15m 25s\n",
      "846:\tlearn: 0.2752459\ttotal: 1h 24m 52s\tremaining: 15m 19s\n",
      "847:\tlearn: 0.2750498\ttotal: 1h 24m 58s\tremaining: 15m 13s\n",
      "848:\tlearn: 0.2747736\ttotal: 1h 25m 4s\tremaining: 15m 7s\n",
      "849:\tlearn: 0.2747115\ttotal: 1h 25m 10s\tremaining: 15m 1s\n",
      "850:\tlearn: 0.2744813\ttotal: 1h 25m 16s\tremaining: 14m 55s\n",
      "851:\tlearn: 0.2743730\ttotal: 1h 25m 22s\tremaining: 14m 49s\n",
      "852:\tlearn: 0.2741937\ttotal: 1h 25m 28s\tremaining: 14m 43s\n",
      "853:\tlearn: 0.2741511\ttotal: 1h 25m 34s\tremaining: 14m 37s\n",
      "854:\tlearn: 0.2736677\ttotal: 1h 25m 40s\tremaining: 14m 31s\n",
      "855:\tlearn: 0.2735247\ttotal: 1h 25m 46s\tremaining: 14m 25s\n",
      "856:\tlearn: 0.2732784\ttotal: 1h 25m 52s\tremaining: 14m 19s\n",
      "857:\tlearn: 0.2731820\ttotal: 1h 25m 58s\tremaining: 14m 13s\n",
      "858:\tlearn: 0.2728019\ttotal: 1h 26m 4s\tremaining: 14m 7s\n",
      "859:\tlearn: 0.2724813\ttotal: 1h 26m 10s\tremaining: 14m 1s\n",
      "860:\tlearn: 0.2724442\ttotal: 1h 26m 16s\tremaining: 13m 55s\n",
      "861:\tlearn: 0.2722925\ttotal: 1h 26m 22s\tremaining: 13m 49s\n",
      "862:\tlearn: 0.2719044\ttotal: 1h 26m 28s\tremaining: 13m 43s\n",
      "863:\tlearn: 0.2717000\ttotal: 1h 26m 34s\tremaining: 13m 37s\n",
      "864:\tlearn: 0.2709324\ttotal: 1h 26m 40s\tremaining: 13m 31s\n",
      "865:\tlearn: 0.2707578\ttotal: 1h 26m 46s\tremaining: 13m 25s\n",
      "866:\tlearn: 0.2706191\ttotal: 1h 26m 51s\tremaining: 13m 19s\n",
      "867:\tlearn: 0.2706033\ttotal: 1h 26m 58s\tremaining: 13m 13s\n",
      "868:\tlearn: 0.2704858\ttotal: 1h 27m 4s\tremaining: 13m 7s\n",
      "869:\tlearn: 0.2703261\ttotal: 1h 27m 10s\tremaining: 13m 1s\n",
      "870:\tlearn: 0.2700863\ttotal: 1h 27m 16s\tremaining: 12m 55s\n",
      "871:\tlearn: 0.2700489\ttotal: 1h 27m 22s\tremaining: 12m 49s\n",
      "872:\tlearn: 0.2700128\ttotal: 1h 27m 28s\tremaining: 12m 43s\n",
      "873:\tlearn: 0.2697952\ttotal: 1h 27m 34s\tremaining: 12m 37s\n",
      "874:\tlearn: 0.2697368\ttotal: 1h 27m 40s\tremaining: 12m 31s\n",
      "875:\tlearn: 0.2696992\ttotal: 1h 27m 46s\tremaining: 12m 25s\n",
      "876:\tlearn: 0.2693278\ttotal: 1h 27m 52s\tremaining: 12m 19s\n",
      "877:\tlearn: 0.2691790\ttotal: 1h 27m 58s\tremaining: 12m 13s\n",
      "878:\tlearn: 0.2691109\ttotal: 1h 28m 4s\tremaining: 12m 7s\n",
      "879:\tlearn: 0.2687743\ttotal: 1h 28m 10s\tremaining: 12m 1s\n",
      "880:\tlearn: 0.2686276\ttotal: 1h 28m 16s\tremaining: 11m 55s\n",
      "881:\tlearn: 0.2684860\ttotal: 1h 28m 22s\tremaining: 11m 49s\n",
      "882:\tlearn: 0.2683853\ttotal: 1h 28m 28s\tremaining: 11m 43s\n",
      "883:\tlearn: 0.2683368\ttotal: 1h 28m 34s\tremaining: 11m 37s\n",
      "884:\tlearn: 0.2678093\ttotal: 1h 28m 40s\tremaining: 11m 31s\n",
      "885:\tlearn: 0.2676619\ttotal: 1h 28m 46s\tremaining: 11m 25s\n",
      "886:\tlearn: 0.2674931\ttotal: 1h 28m 52s\tremaining: 11m 19s\n",
      "887:\tlearn: 0.2672994\ttotal: 1h 28m 58s\tremaining: 11m 13s\n",
      "888:\tlearn: 0.2667913\ttotal: 1h 29m 4s\tremaining: 11m 7s\n",
      "889:\tlearn: 0.2666546\ttotal: 1h 29m 10s\tremaining: 11m 1s\n",
      "890:\tlearn: 0.2665362\ttotal: 1h 29m 16s\tremaining: 10m 55s\n",
      "891:\tlearn: 0.2663423\ttotal: 1h 29m 22s\tremaining: 10m 49s\n",
      "892:\tlearn: 0.2660537\ttotal: 1h 29m 27s\tremaining: 10m 43s\n",
      "893:\tlearn: 0.2659642\ttotal: 1h 29m 33s\tremaining: 10m 37s\n",
      "894:\tlearn: 0.2654703\ttotal: 1h 29m 39s\tremaining: 10m 31s\n",
      "895:\tlearn: 0.2653181\ttotal: 1h 29m 45s\tremaining: 10m 25s\n",
      "896:\tlearn: 0.2652549\ttotal: 1h 29m 51s\tremaining: 10m 19s\n",
      "897:\tlearn: 0.2647619\ttotal: 1h 29m 57s\tremaining: 10m 13s\n",
      "898:\tlearn: 0.2645748\ttotal: 1h 30m 3s\tremaining: 10m 7s\n",
      "899:\tlearn: 0.2645453\ttotal: 1h 30m 9s\tremaining: 10m 1s\n",
      "900:\tlearn: 0.2644158\ttotal: 1h 30m 15s\tremaining: 9m 55s\n",
      "901:\tlearn: 0.2643597\ttotal: 1h 30m 22s\tremaining: 9m 49s\n",
      "902:\tlearn: 0.2638666\ttotal: 1h 30m 27s\tremaining: 9m 43s\n",
      "903:\tlearn: 0.2633742\ttotal: 1h 30m 33s\tremaining: 9m 37s\n",
      "904:\tlearn: 0.2632767\ttotal: 1h 30m 39s\tremaining: 9m 31s\n",
      "905:\tlearn: 0.2630002\ttotal: 1h 30m 45s\tremaining: 9m 25s\n",
      "906:\tlearn: 0.2627666\ttotal: 1h 30m 51s\tremaining: 9m 18s\n",
      "907:\tlearn: 0.2625038\ttotal: 1h 30m 57s\tremaining: 9m 12s\n",
      "908:\tlearn: 0.2622453\ttotal: 1h 31m 3s\tremaining: 9m 6s\n",
      "909:\tlearn: 0.2622094\ttotal: 1h 31m 9s\tremaining: 9m\n",
      "910:\tlearn: 0.2621380\ttotal: 1h 31m 15s\tremaining: 8m 54s\n",
      "911:\tlearn: 0.2620272\ttotal: 1h 31m 21s\tremaining: 8m 48s\n",
      "912:\tlearn: 0.2620155\ttotal: 1h 31m 27s\tremaining: 8m 42s\n",
      "913:\tlearn: 0.2619730\ttotal: 1h 31m 33s\tremaining: 8m 36s\n",
      "914:\tlearn: 0.2618334\ttotal: 1h 31m 39s\tremaining: 8m 30s\n",
      "915:\tlearn: 0.2613072\ttotal: 1h 31m 45s\tremaining: 8m 24s\n",
      "916:\tlearn: 0.2612536\ttotal: 1h 31m 51s\tremaining: 8m 18s\n",
      "917:\tlearn: 0.2609462\ttotal: 1h 31m 57s\tremaining: 8m 12s\n",
      "918:\tlearn: 0.2608481\ttotal: 1h 32m 3s\tremaining: 8m 6s\n",
      "919:\tlearn: 0.2607899\ttotal: 1h 32m 9s\tremaining: 8m\n",
      "920:\tlearn: 0.2607226\ttotal: 1h 32m 15s\tremaining: 7m 54s\n",
      "921:\tlearn: 0.2604171\ttotal: 1h 32m 21s\tremaining: 7m 48s\n",
      "922:\tlearn: 0.2599823\ttotal: 1h 32m 27s\tremaining: 7m 42s\n",
      "923:\tlearn: 0.2599800\ttotal: 1h 32m 33s\tremaining: 7m 36s\n",
      "924:\tlearn: 0.2597763\ttotal: 1h 32m 39s\tremaining: 7m 30s\n",
      "925:\tlearn: 0.2596807\ttotal: 1h 32m 45s\tremaining: 7m 24s\n",
      "926:\tlearn: 0.2593175\ttotal: 1h 32m 51s\tremaining: 7m 18s\n",
      "927:\tlearn: 0.2589439\ttotal: 1h 32m 57s\tremaining: 7m 12s\n",
      "928:\tlearn: 0.2586785\ttotal: 1h 33m 3s\tremaining: 7m 6s\n",
      "929:\tlearn: 0.2585003\ttotal: 1h 33m 9s\tremaining: 7m\n",
      "930:\tlearn: 0.2583887\ttotal: 1h 33m 15s\tremaining: 6m 54s\n",
      "931:\tlearn: 0.2582784\ttotal: 1h 33m 21s\tremaining: 6m 48s\n",
      "932:\tlearn: 0.2580773\ttotal: 1h 33m 27s\tremaining: 6m 42s\n",
      "933:\tlearn: 0.2578418\ttotal: 1h 33m 33s\tremaining: 6m 36s\n",
      "934:\tlearn: 0.2577521\ttotal: 1h 33m 39s\tremaining: 6m 30s\n",
      "935:\tlearn: 0.2577318\ttotal: 1h 33m 45s\tremaining: 6m 24s\n",
      "936:\tlearn: 0.2574970\ttotal: 1h 33m 51s\tremaining: 6m 18s\n",
      "937:\tlearn: 0.2572181\ttotal: 1h 33m 57s\tremaining: 6m 12s\n",
      "938:\tlearn: 0.2570515\ttotal: 1h 34m 3s\tremaining: 6m 6s\n",
      "939:\tlearn: 0.2570304\ttotal: 1h 34m 9s\tremaining: 6m\n",
      "940:\tlearn: 0.2567204\ttotal: 1h 34m 15s\tremaining: 5m 54s\n",
      "941:\tlearn: 0.2564034\ttotal: 1h 34m 21s\tremaining: 5m 48s\n",
      "942:\tlearn: 0.2562544\ttotal: 1h 34m 27s\tremaining: 5m 42s\n",
      "943:\tlearn: 0.2557127\ttotal: 1h 34m 33s\tremaining: 5m 36s\n",
      "944:\tlearn: 0.2556892\ttotal: 1h 34m 39s\tremaining: 5m 30s\n",
      "945:\tlearn: 0.2556173\ttotal: 1h 34m 45s\tremaining: 5m 24s\n",
      "946:\tlearn: 0.2552751\ttotal: 1h 34m 51s\tremaining: 5m 18s\n",
      "947:\tlearn: 0.2550881\ttotal: 1h 34m 57s\tremaining: 5m 12s\n",
      "948:\tlearn: 0.2548781\ttotal: 1h 35m 3s\tremaining: 5m 6s\n",
      "949:\tlearn: 0.2548015\ttotal: 1h 35m 9s\tremaining: 5m\n",
      "950:\tlearn: 0.2546663\ttotal: 1h 35m 15s\tremaining: 4m 54s\n",
      "951:\tlearn: 0.2546013\ttotal: 1h 35m 21s\tremaining: 4m 48s\n",
      "952:\tlearn: 0.2543891\ttotal: 1h 35m 27s\tremaining: 4m 42s\n",
      "953:\tlearn: 0.2542117\ttotal: 1h 35m 33s\tremaining: 4m 36s\n",
      "954:\tlearn: 0.2540559\ttotal: 1h 35m 39s\tremaining: 4m 30s\n",
      "955:\tlearn: 0.2538121\ttotal: 1h 35m 45s\tremaining: 4m 24s\n",
      "956:\tlearn: 0.2537036\ttotal: 1h 35m 51s\tremaining: 4m 18s\n",
      "957:\tlearn: 0.2535123\ttotal: 1h 35m 57s\tremaining: 4m 12s\n",
      "958:\tlearn: 0.2533735\ttotal: 1h 36m 3s\tremaining: 4m 6s\n",
      "959:\tlearn: 0.2529385\ttotal: 1h 36m 9s\tremaining: 4m\n",
      "960:\tlearn: 0.2528197\ttotal: 1h 36m 15s\tremaining: 3m 54s\n",
      "961:\tlearn: 0.2525425\ttotal: 1h 36m 21s\tremaining: 3m 48s\n",
      "962:\tlearn: 0.2524261\ttotal: 1h 36m 27s\tremaining: 3m 42s\n",
      "963:\tlearn: 0.2524105\ttotal: 1h 36m 33s\tremaining: 3m 36s\n",
      "964:\tlearn: 0.2522979\ttotal: 1h 36m 39s\tremaining: 3m 30s\n",
      "965:\tlearn: 0.2521870\ttotal: 1h 36m 44s\tremaining: 3m 24s\n",
      "966:\tlearn: 0.2521031\ttotal: 1h 36m 50s\tremaining: 3m 18s\n",
      "967:\tlearn: 0.2519757\ttotal: 1h 36m 56s\tremaining: 3m 12s\n",
      "968:\tlearn: 0.2516989\ttotal: 1h 37m 2s\tremaining: 3m 6s\n",
      "969:\tlearn: 0.2516702\ttotal: 1h 37m 9s\tremaining: 3m\n",
      "970:\tlearn: 0.2515437\ttotal: 1h 37m 14s\tremaining: 2m 54s\n",
      "971:\tlearn: 0.2511753\ttotal: 1h 37m 20s\tremaining: 2m 48s\n",
      "972:\tlearn: 0.2510623\ttotal: 1h 37m 26s\tremaining: 2m 42s\n",
      "973:\tlearn: 0.2510362\ttotal: 1h 37m 32s\tremaining: 2m 36s\n",
      "974:\tlearn: 0.2507288\ttotal: 1h 37m 38s\tremaining: 2m 30s\n",
      "975:\tlearn: 0.2506965\ttotal: 1h 37m 44s\tremaining: 2m 24s\n",
      "976:\tlearn: 0.2504118\ttotal: 1h 37m 50s\tremaining: 2m 18s\n",
      "977:\tlearn: 0.2504070\ttotal: 1h 37m 56s\tremaining: 2m 12s\n",
      "978:\tlearn: 0.2501876\ttotal: 1h 38m 2s\tremaining: 2m 6s\n",
      "979:\tlearn: 0.2499764\ttotal: 1h 38m 8s\tremaining: 2m\n",
      "980:\tlearn: 0.2498399\ttotal: 1h 38m 14s\tremaining: 1m 54s\n",
      "981:\tlearn: 0.2497545\ttotal: 1h 38m 20s\tremaining: 1m 48s\n",
      "982:\tlearn: 0.2494079\ttotal: 1h 38m 26s\tremaining: 1m 42s\n",
      "983:\tlearn: 0.2492688\ttotal: 1h 38m 32s\tremaining: 1m 36s\n",
      "984:\tlearn: 0.2492337\ttotal: 1h 38m 38s\tremaining: 1m 30s\n",
      "985:\tlearn: 0.2488643\ttotal: 1h 38m 44s\tremaining: 1m 24s\n",
      "986:\tlearn: 0.2487400\ttotal: 1h 38m 50s\tremaining: 1m 18s\n",
      "987:\tlearn: 0.2486315\ttotal: 1h 38m 56s\tremaining: 1m 12s\n",
      "988:\tlearn: 0.2484814\ttotal: 1h 39m 2s\tremaining: 1m 6s\n",
      "989:\tlearn: 0.2484408\ttotal: 1h 39m 8s\tremaining: 1m\n",
      "990:\tlearn: 0.2483284\ttotal: 1h 39m 14s\tremaining: 54.1s\n",
      "991:\tlearn: 0.2480474\ttotal: 1h 39m 20s\tremaining: 48.1s\n",
      "992:\tlearn: 0.2477240\ttotal: 1h 39m 26s\tremaining: 42.1s\n",
      "993:\tlearn: 0.2473935\ttotal: 1h 39m 32s\tremaining: 36.1s\n",
      "994:\tlearn: 0.2473571\ttotal: 1h 39m 38s\tremaining: 30s\n",
      "995:\tlearn: 0.2472338\ttotal: 1h 39m 44s\tremaining: 24s\n",
      "996:\tlearn: 0.2472132\ttotal: 1h 39m 51s\tremaining: 18s\n",
      "997:\tlearn: 0.2471749\ttotal: 1h 39m 57s\tremaining: 12s\n",
      "998:\tlearn: 0.2470982\ttotal: 1h 40m 3s\tremaining: 6.01s\n",
      "999:\tlearn: 0.2466908\ttotal: 1h 40m 9s\tremaining: 0us\n",
      "Time Taken To Train CatBoost on dataset 3 : 6012.212338685989\n",
      "Accuracy: 0.9319076497499734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6633    0.4610    0.5439       141\n",
      "           1     0.8561    1.0000    0.9225       119\n",
      "           2     0.9455    0.7820    0.8560       133\n",
      "           3     1.0000    1.0000    1.0000       128\n",
      "           4     0.8770    0.7810    0.8263       137\n",
      "           5     0.9268    0.9268    0.9268       123\n",
      "           6     0.9250    0.9407    0.9328       118\n",
      "           7     0.9856    1.0000    0.9928       137\n",
      "           8     0.9580    1.0000    0.9786       137\n",
      "           9     0.9797    1.0000    0.9898       145\n",
      "          10     0.9032    0.9106    0.9069       123\n",
      "          11     0.8182    0.8182    0.8182       121\n",
      "          12     0.9126    0.8468    0.8785       111\n",
      "          13     0.9833    1.0000    0.9916       118\n",
      "          14     0.9851    1.0000    0.9925       132\n",
      "          15     0.9847    1.0000    0.9923       129\n",
      "          16     0.9783    1.0000    0.9890       135\n",
      "          17     0.8433    0.8433    0.8433       134\n",
      "          18     0.9333    0.9333    0.9333       120\n",
      "          19     0.9789    1.0000    0.9893       139\n",
      "          20     0.9922    1.0000    0.9961       128\n",
      "          21     0.9704    1.0000    0.9850       131\n",
      "          22     0.9922    0.9478    0.9695       134\n",
      "          23     0.8830    0.6748    0.7650       123\n",
      "          24     0.8837    0.9620    0.9212        79\n",
      "          25     0.8258    0.9624    0.8889       133\n",
      "          26     1.0000    1.0000    1.0000       125\n",
      "          27     0.9394    0.9612    0.9502       129\n",
      "          28     0.9841    0.9612    0.9725       129\n",
      "          29     1.0000    1.0000    1.0000       133\n",
      "          30     0.9421    1.0000    0.9702       114\n",
      "          31     1.0000    1.0000    1.0000       147\n",
      "          32     1.0000    1.0000    1.0000       132\n",
      "          33     0.9667    1.0000    0.9831       116\n",
      "          34     0.9806    0.9266    0.9528       109\n",
      "          35     0.9638    1.0000    0.9815       133\n",
      "          36     0.9868    1.0000    0.9933       149\n",
      "          37     0.9851    1.0000    0.9925       132\n",
      "          38     1.0000    1.0000    1.0000       146\n",
      "          39     0.9695    0.9407    0.9549       135\n",
      "          40     0.9806    0.8279    0.8978       122\n",
      "          41     1.0000    1.0000    1.0000       133\n",
      "          42     0.7000    0.9573    0.8087       117\n",
      "          43     0.9643    0.8438    0.9000        32\n",
      "          44     1.0000    1.0000    1.0000       134\n",
      "          45     0.9216    0.6667    0.7737       141\n",
      "          46     0.9845    1.0000    0.9922       127\n",
      "          47     1.0000    1.0000    1.0000       109\n",
      "          48     1.0000    1.0000    1.0000       141\n",
      "          49     1.0000    1.0000    1.0000       114\n",
      "          50     1.0000    1.0000    1.0000       134\n",
      "          51     1.0000    1.0000    1.0000       122\n",
      "          52     1.0000    1.0000    1.0000       138\n",
      "          53     1.0000    0.5390    0.7005       141\n",
      "          54     0.9929    1.0000    0.9964       140\n",
      "          55     1.0000    1.0000    1.0000       133\n",
      "          56     0.8750    0.4148    0.5628       135\n",
      "          57     0.9732    0.8385    0.9008       130\n",
      "          58     1.0000    1.0000    1.0000       127\n",
      "          59     0.9907    1.0000    0.9953       107\n",
      "          60     1.0000    1.0000    1.0000       117\n",
      "          61     1.0000    1.0000    1.0000       131\n",
      "          62     1.0000    1.0000    1.0000       127\n",
      "          63     1.0000    1.0000    1.0000       149\n",
      "          64     1.0000    1.0000    1.0000       126\n",
      "          65     1.0000    1.0000    1.0000       126\n",
      "          66     0.9917    1.0000    0.9959       120\n",
      "          67     0.9122    1.0000    0.9541       135\n",
      "          68     1.0000    1.0000    1.0000       126\n",
      "          69     1.0000    1.0000    1.0000       128\n",
      "          70     0.9697    1.0000    0.9846       128\n",
      "          71     1.0000    1.0000    1.0000       122\n",
      "          72     0.9286    0.5200    0.6667       125\n",
      "          73     0.2712    0.7920    0.4041       125\n",
      "\n",
      "    accuracy                         0.9319      9399\n",
      "   macro avg     0.9481    0.9322    0.9340      9399\n",
      "weighted avg     0.9489    0.9319    0.9341      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.947016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoost Classifier</td>\n",
       "      <td>0.874455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>0.882222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost Classifier</td>\n",
       "      <td>0.931908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349\n",
       "1                   Bagging Classifier  0.947016\n",
       "1             GradientBoost Classifier  0.874455\n",
       "1                   XGBoost Classifier  0.882222\n",
       "1                  CatBoost Classifier  0.931908"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "data_3_resampled_cb = CatBoostClassifier() \n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_cb.fit(X_train,y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train CatBoost on dataset 3 : {end - start}\")\n",
    "\n",
    "y_pred=data_3_resampled_cb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_cb.predict(X_test), digits=4))\n",
    "\n",
    "results_data_3_resampled_cb= pd.DataFrame({'Model':['CatBoost Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred)]},index={'1'})\n",
    "results_ML_Models  = pd.concat([results_ML_Models, results_data_3_resampled_cb])\n",
    "results_ML_Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XQlvwDC317M"
   },
   "source": [
    "# ML Model 6 :  SVM & SGD Classifier (Dataset 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-isinJl1317N"
   },
   "source": [
    "Note: A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize the new text. It is a fast and dependable classification algorithm that performs very well with a limited amount of data. A popular algorithm for this technique is penalized SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1019727,
     "status": "ok",
     "timestamp": 1598782414548,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "R34vAyRT317O",
    "outputId": "4af8db96-fadd-4b8d-d05b-112d4c170917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train SVC on dataset 3 : 636.471348285675\n",
      "Accuracy: 0.27981700180870306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       141\n",
      "           1     0.0000    0.0000    0.0000       119\n",
      "           2     0.9444    0.2556    0.4024       133\n",
      "           3     0.0000    0.0000    0.0000       128\n",
      "           4     0.0000    0.0000    0.0000       137\n",
      "           5     0.0000    0.0000    0.0000       123\n",
      "           6     0.0000    0.0000    0.0000       118\n",
      "           7     0.0000    0.0000    0.0000       137\n",
      "           8     0.0000    0.0000    0.0000       137\n",
      "           9     0.0000    0.0000    0.0000       145\n",
      "          10     0.0000    0.0000    0.0000       123\n",
      "          11     0.0000    0.0000    0.0000       121\n",
      "          12     0.0000    0.0000    0.0000       111\n",
      "          13     0.0000    0.0000    0.0000       118\n",
      "          14     0.0000    0.0000    0.0000       132\n",
      "          15     0.0000    0.0000    0.0000       129\n",
      "          16     0.0000    0.0000    0.0000       135\n",
      "          17     0.0000    0.0000    0.0000       134\n",
      "          18     0.0000    0.0000    0.0000       120\n",
      "          19     0.0000    0.0000    0.0000       139\n",
      "          20     0.0000    0.0000    0.0000       128\n",
      "          21     0.0000    0.0000    0.0000       131\n",
      "          22     0.0000    0.0000    0.0000       134\n",
      "          23     0.0000    0.0000    0.0000       123\n",
      "          24     0.0000    0.0000    0.0000        79\n",
      "          25     0.0000    0.0000    0.0000       133\n",
      "          26     1.0000    1.0000    1.0000       125\n",
      "          27     0.0000    0.0000    0.0000       129\n",
      "          28     0.0000    0.0000    0.0000       129\n",
      "          29     1.0000    1.0000    1.0000       133\n",
      "          30     0.0000    0.0000    0.0000       114\n",
      "          31     0.0000    0.0000    0.0000       147\n",
      "          32     1.0000    1.0000    1.0000       132\n",
      "          33     1.0000    0.1810    0.3066       116\n",
      "          34     0.0000    0.0000    0.0000       109\n",
      "          35     0.0000    0.0000    0.0000       133\n",
      "          36     0.0000    0.0000    0.0000       149\n",
      "          37     0.0000    0.0000    0.0000       132\n",
      "          38     1.0000    0.3767    0.5473       146\n",
      "          39     0.0000    0.0000    0.0000       135\n",
      "          40     0.0000    0.0000    0.0000       122\n",
      "          41     1.0000    0.1353    0.2384       133\n",
      "          42     0.3239    0.1966    0.2447       117\n",
      "          43     0.0000    0.0000    0.0000        32\n",
      "          44     0.0000    0.0000    0.0000       134\n",
      "          45     0.0000    0.0000    0.0000       141\n",
      "          46     0.0000    0.0000    0.0000       127\n",
      "          47     1.0000    0.1651    0.2835       109\n",
      "          48     0.0000    0.0000    0.0000       141\n",
      "          49     0.0000    0.0000    0.0000       114\n",
      "          50     1.0000    1.0000    1.0000       134\n",
      "          51     0.0000    0.0000    0.0000       122\n",
      "          52     1.0000    1.0000    1.0000       138\n",
      "          53     1.0000    0.5390    0.7005       141\n",
      "          54     1.0000    1.0000    1.0000       140\n",
      "          55     0.0000    0.0000    0.0000       133\n",
      "          56     0.0000    0.0000    0.0000       135\n",
      "          57     0.9545    0.1615    0.2763       130\n",
      "          58     1.0000    1.0000    1.0000       127\n",
      "          59     0.0173    1.0000    0.0340       107\n",
      "          60     1.0000    1.0000    1.0000       117\n",
      "          61     1.0000    1.0000    1.0000       131\n",
      "          62     0.0000    0.0000    0.0000       127\n",
      "          63     1.0000    1.0000    1.0000       149\n",
      "          64     1.0000    1.0000    1.0000       126\n",
      "          65     1.0000    1.0000    1.0000       126\n",
      "          66     1.0000    1.0000    1.0000       120\n",
      "          67     0.0000    0.0000    0.0000       135\n",
      "          68     1.0000    1.0000    1.0000       126\n",
      "          69     1.0000    1.0000    1.0000       128\n",
      "          70     0.9697    1.0000    0.9846       128\n",
      "          71     1.0000    1.0000    1.0000       122\n",
      "          72     0.0000    0.0000    0.0000       125\n",
      "          73     0.1735    0.4400    0.2489       125\n",
      "\n",
      "    accuracy                         0.2798      9399\n",
      "   macro avg     0.3295    0.2764    0.2739      9399\n",
      "weighted avg     0.3356    0.2798    0.2798      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.947016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoost Classifier</td>\n",
       "      <td>0.874455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>0.882222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost Classifier</td>\n",
       "      <td>0.931908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>0.279817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349\n",
       "1                   Bagging Classifier  0.947016\n",
       "1             GradientBoost Classifier  0.874455\n",
       "1                   XGBoost Classifier  0.882222\n",
       "1                  CatBoost Classifier  0.931908\n",
       "2                       SVM Classifier  0.279817"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "data_3_resampled_SVC = svm.SVC(kernel='rbf')\n",
    "\n",
    "\n",
    "Cs = [0.01 ]  # [0.01, 0.1 ] #Cs = [0.01, 0.1, 1, 10]\n",
    "gammas = [0.001 ] # [0.001, 0.01 ] #gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "\n",
    "#start = time.time()\n",
    "#grid_search = GridSearchCV(data_3_resampled_SVC, param_grid)\n",
    "#grid_search.fit(X_train,y_train)\n",
    "#time.sleep(1)\n",
    "#end = time.time()\n",
    "#print(f\"Time Taken To Get Best Parameters For SVC on dataset 3 dataset with Grid Search: {end - start}\")\n",
    "\n",
    "#data_3_resampled_SVC = svm.SVC(**grid_search.best_params_)\n",
    "data_3_resampled_SVC = svm.SVC(kernel='rbf', C=.01, gamma=.001)\n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_SVC.fit(X_train, y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train SVC on dataset 3 : {end - start}\")\n",
    "\n",
    "y_pred_SVC = data_3_resampled_SVC.predict(X_test) #predictig using SVC Classifier\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_SVC))\n",
    "\n",
    "report = classification_report(y_test, data_3_resampled_SVC.predict(X_test), digits=4)\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_SVC.predict(X_test), digits=4))\n",
    "\n",
    "results_data_3_resampled_SVC = pd.DataFrame({'Model':['SVM Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred_SVC)]},index={'2'})\n",
    "results_ML_Models = pd.concat([results_ML_Models, results_data_3_resampled_SVC])\n",
    "results_ML_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60528,
     "status": "ok",
     "timestamp": 1598782560597,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "Yp_ZIz8R317R",
    "outputId": "8888c650-5a14-45aa-c4f5-d201660e10af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train SGD on dataset 3 : 142.51342153549194\n",
      "Accuracy: 0.2743908926481541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       141\n",
      "           1     0.0727    0.4034    0.1232       119\n",
      "           2     0.2308    0.0226    0.0411       133\n",
      "           3     0.0742    0.2656    0.1160       128\n",
      "           4     0.3333    0.0219    0.0411       137\n",
      "           5     0.2817    0.1626    0.2062       123\n",
      "           6     0.0447    0.0678    0.0539       118\n",
      "           7     0.0510    0.2701    0.0857       137\n",
      "           8     0.4000    0.0584    0.1019       137\n",
      "           9     0.0455    0.0483    0.0468       145\n",
      "          10     0.0703    0.1463    0.0950       123\n",
      "          11     0.0385    0.0165    0.0231       121\n",
      "          12     0.2353    0.1081    0.1481       111\n",
      "          13     0.2568    0.3983    0.3123       118\n",
      "          14     0.2121    0.2121    0.2121       132\n",
      "          15     0.0375    0.3876    0.0684       129\n",
      "          16     0.6176    0.1556    0.2485       135\n",
      "          17     0.3684    0.0522    0.0915       134\n",
      "          18     0.1639    0.0833    0.1105       120\n",
      "          19     0.4915    0.2086    0.2929       139\n",
      "          20     0.1230    0.1797    0.1460       128\n",
      "          21     0.0783    0.1298    0.0977       131\n",
      "          22     0.0681    0.1194    0.0867       134\n",
      "          23     0.2857    0.0488    0.0833       123\n",
      "          24     1.0000    0.1392    0.2444        79\n",
      "          25     0.0523    0.0602    0.0559       133\n",
      "          26     0.8718    0.2720    0.4146       125\n",
      "          27     0.0925    0.2791    0.1390       129\n",
      "          28     0.0833    0.0233    0.0364       129\n",
      "          29     0.9708    1.0000    0.9852       133\n",
      "          30     0.6667    0.1404    0.2319       114\n",
      "          31     0.2438    0.4694    0.3209       147\n",
      "          32     0.6691    0.6894    0.6791       132\n",
      "          33     0.2338    0.3103    0.2667       116\n",
      "          34     0.5652    0.1193    0.1970       109\n",
      "          35     0.4167    0.0376    0.0690       133\n",
      "          36     0.6957    0.1074    0.1860       149\n",
      "          37     0.0952    0.0909    0.0930       132\n",
      "          38     0.0000    0.0000    0.0000       146\n",
      "          39     0.8814    0.3852    0.5361       135\n",
      "          40     0.7391    0.1393    0.2345       122\n",
      "          41     0.9000    0.1353    0.2353       133\n",
      "          42     0.6800    0.1453    0.2394       117\n",
      "          43     1.0000    0.1250    0.2222        32\n",
      "          44     0.0000    0.0000    0.0000       134\n",
      "          45     1.0000    0.0496    0.0946       141\n",
      "          46     0.0787    0.2362    0.1181       127\n",
      "          47     0.3678    0.2936    0.3265       109\n",
      "          48     0.5200    0.0922    0.1566       141\n",
      "          49     1.0000    0.3684    0.5385       114\n",
      "          50     0.8194    0.4403    0.5728       134\n",
      "          51     0.0000    0.0000    0.0000       122\n",
      "          52     0.9891    0.6594    0.7913       138\n",
      "          53     0.9500    0.5390    0.6878       141\n",
      "          54     0.9333    0.6000    0.7304       140\n",
      "          55     0.8000    0.7820    0.7909       133\n",
      "          56     1.0000    0.0148    0.0292       135\n",
      "          57     1.0000    0.0692    0.1295       130\n",
      "          58     1.0000    1.0000    1.0000       127\n",
      "          59     0.0829    0.3178    0.1315       107\n",
      "          60     0.0000    0.0000    0.0000       117\n",
      "          61     1.0000    1.0000    1.0000       131\n",
      "          62     0.9286    0.2047    0.3355       127\n",
      "          63     0.0000    0.0000    0.0000       149\n",
      "          64     0.8456    1.0000    0.9164       126\n",
      "          65     0.0000    0.0000    0.0000       126\n",
      "          66     0.9917    1.0000    0.9959       120\n",
      "          67     1.0000    0.0370    0.0714       135\n",
      "          68     0.8630    1.0000    0.9265       126\n",
      "          69     1.0000    0.4609    0.6310       128\n",
      "          70     0.4397    0.4844    0.4610       128\n",
      "          71     1.0000    1.0000    1.0000       122\n",
      "          72     0.9000    0.2160    0.3484       125\n",
      "          73     0.2128    0.1600    0.1826       125\n",
      "\n",
      "    accuracy                         0.2744      9399\n",
      "   macro avg     0.4752    0.2738    0.2863      9399\n",
      "weighted avg     0.4686    0.2744    0.2862      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.947016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoost Classifier</td>\n",
       "      <td>0.874455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>0.882222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost Classifier</td>\n",
       "      <td>0.931908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>0.279817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.274391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349\n",
       "1                   Bagging Classifier  0.947016\n",
       "1             GradientBoost Classifier  0.874455\n",
       "1                   XGBoost Classifier  0.882222\n",
       "1                  CatBoost Classifier  0.931908\n",
       "2                       SVM Classifier  0.279817\n",
       "2                       SGD Classifier  0.274391"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "data_3_resampled_SGD = SGDClassifier()\n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_SGD.fit(X_train, y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train SGD on dataset 3 : {end - start}\")\n",
    "\n",
    "y_pred_SGD = data_3_resampled_SGD.predict(X_test) #predicting using SGD Classifier\n",
    "\n",
    "#accuracy_score(y_test,y_pred_SGD) #accuracy score of two models\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_SGD))\n",
    "print(classification_report(y_test, data_3_resampled_SGD.predict(X_test), digits=4))\n",
    "\n",
    "results_data_3_resampled_SGD = pd.DataFrame({'Model':['SGD Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred_SGD)]},index={'2'})\n",
    "results_ML_Models = pd.concat([results_ML_Models, results_data_3_resampled_SGD])\n",
    "results_ML_Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyN35STs317U"
   },
   "source": [
    "# ML Model 7: Naive Bayes (Dataset 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9991,
     "status": "ok",
     "timestamp": 1598782591578,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "-GuCUhOO317V",
    "outputId": "bfff4a09-c171-4cc6-c2cb-c7aaa0e002b9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       141\n",
      "           1     0.4528    0.2017    0.2791       119\n",
      "           2     0.0149    0.0075    0.0100       133\n",
      "           3     0.1786    0.1172    0.1415       128\n",
      "           4     1.0000    0.0073    0.0145       137\n",
      "           5     0.0364    0.0163    0.0225       123\n",
      "           6     0.0000    0.0000    0.0000       118\n",
      "           7     0.3833    0.1679    0.2335       137\n",
      "           8     0.4000    0.0292    0.0544       137\n",
      "           9     0.1244    0.1931    0.1514       145\n",
      "          10     0.0000    0.0000    0.0000       123\n",
      "          11     0.0000    0.0000    0.0000       121\n",
      "          12     0.0000    0.0000    0.0000       111\n",
      "          13     0.8333    0.0847    0.1538       118\n",
      "          14     0.2553    0.0909    0.1341       132\n",
      "          15     0.0000    0.0000    0.0000       129\n",
      "          16     0.2386    0.1556    0.1883       135\n",
      "          17     0.0785    0.3060    0.1250       134\n",
      "          18     1.0000    0.0167    0.0328       120\n",
      "          19     0.3636    0.0863    0.1395       139\n",
      "          20     0.1020    0.0781    0.0885       128\n",
      "          21     0.0000    0.0000    0.0000       131\n",
      "          22     0.1111    0.0224    0.0373       134\n",
      "          23     0.0000    0.0000    0.0000       123\n",
      "          24     0.3846    0.0633    0.1087        79\n",
      "          25     0.1111    0.0075    0.0141       133\n",
      "          26     0.2083    0.4800    0.2906       125\n",
      "          27     0.0000    0.0000    0.0000       129\n",
      "          28     0.0000    0.0000    0.0000       129\n",
      "          29     0.3994    1.0000    0.5708       133\n",
      "          30     0.6400    0.1404    0.2302       114\n",
      "          31     0.0000    0.0000    0.0000       147\n",
      "          32     0.7917    0.2879    0.4222       132\n",
      "          33     0.9200    0.1983    0.3262       116\n",
      "          34     0.0000    0.0000    0.0000       109\n",
      "          35     0.1905    0.0301    0.0519       133\n",
      "          36     1.0000    0.0134    0.0265       149\n",
      "          37     0.0000    0.0000    0.0000       132\n",
      "          38     0.3509    0.6370    0.4526       146\n",
      "          39     0.2941    0.0741    0.1183       135\n",
      "          40     0.3529    0.0984    0.1538       122\n",
      "          41     0.2812    1.0000    0.4389       133\n",
      "          42     0.5333    0.0684    0.1212       117\n",
      "          43     1.0000    0.1250    0.2222        32\n",
      "          44     0.2552    0.5448    0.3476       134\n",
      "          45     0.6667    0.0142    0.0278       141\n",
      "          46     0.9130    0.1654    0.2800       127\n",
      "          47     0.2874    0.4587    0.3534       109\n",
      "          48     0.4889    0.3121    0.3810       141\n",
      "          49     0.3229    0.2719    0.2952       114\n",
      "          50     0.7836    1.0000    0.8787       134\n",
      "          51     0.1466    0.5082    0.2275       122\n",
      "          52     0.7879    0.3768    0.5098       138\n",
      "          53     0.5507    0.5390    0.5448       141\n",
      "          54     0.3401    0.6000    0.4341       140\n",
      "          55     0.8611    0.2331    0.3669       133\n",
      "          56     0.3333    0.0074    0.0145       135\n",
      "          57     0.5909    0.2000    0.2989       130\n",
      "          58     0.4847    1.0000    0.6530       127\n",
      "          59     1.0000    0.0561    0.1062       107\n",
      "          60     0.4149    1.0000    0.5865       117\n",
      "          61     0.8506    1.0000    0.9193       131\n",
      "          62     0.3504    0.3780    0.3636       127\n",
      "          63     0.1473    0.7248    0.2449       149\n",
      "          64     0.4131    1.0000    0.5847       126\n",
      "          65     0.1620    1.0000    0.2788       126\n",
      "          66     0.7692    1.0000    0.8696       120\n",
      "          67     0.0106    0.0074    0.0087       135\n",
      "          68     0.2890    1.0000    0.4484       126\n",
      "          69     0.8429    0.4609    0.5960       128\n",
      "          70     0.1345    1.0000    0.2370       128\n",
      "          71     0.7176    1.0000    0.8356       122\n",
      "          72     0.4921    0.2480    0.3298       125\n",
      "          73     0.0000    0.0000    0.0000       125\n",
      "\n",
      "    accuracy                         0.2994      9399\n",
      "   macro avg     0.3708    0.2961    0.2348      9399\n",
      "weighted avg     0.3632    0.2994    0.2353      9399\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       141\n",
      "           1     0.6111    0.0924    0.1606       119\n",
      "           2     0.0000    0.0000    0.0000       133\n",
      "           3     0.3333    0.0859    0.1366       128\n",
      "           4     0.0000    0.0000    0.0000       137\n",
      "           5     0.0325    0.0407    0.0361       123\n",
      "           6     0.0000    0.0000    0.0000       118\n",
      "           7     0.1667    0.1679    0.1673       137\n",
      "           8     0.0849    0.0657    0.0741       137\n",
      "           9     0.0000    0.0000    0.0000       145\n",
      "          10     1.0000    0.0081    0.0161       123\n",
      "          11     0.0000    0.0000    0.0000       121\n",
      "          12     0.0000    0.0000    0.0000       111\n",
      "          13     0.1111    0.0169    0.0294       118\n",
      "          14     0.5000    0.0152    0.0294       132\n",
      "          15     0.0000    0.0000    0.0000       129\n",
      "          16     0.2727    0.0222    0.0411       135\n",
      "          17     0.0282    0.8433    0.0546       134\n",
      "          18     0.0000    0.0000    0.0000       120\n",
      "          19     0.1277    0.0863    0.1030       139\n",
      "          20     0.0000    0.0000    0.0000       128\n",
      "          21     0.0000    0.0000    0.0000       131\n",
      "          22     0.0500    0.0224    0.0309       134\n",
      "          23     0.0000    0.0000    0.0000       123\n",
      "          24     0.0000    0.0000    0.0000        79\n",
      "          25     0.0000    0.0000    0.0000       133\n",
      "          26     0.4146    0.2720    0.3285       125\n",
      "          27     0.0100    0.0543    0.0169       129\n",
      "          28     0.0361    0.0465    0.0407       129\n",
      "          29     0.0000    0.0000    0.0000       133\n",
      "          30     0.5000    0.1404    0.2192       114\n",
      "          31     0.0000    0.0000    0.0000       147\n",
      "          32     0.6909    0.2879    0.4064       132\n",
      "          33     1.0000    0.0345    0.0667       116\n",
      "          34     0.0000    0.0000    0.0000       109\n",
      "          35     0.2059    0.0526    0.0838       133\n",
      "          36     0.0000    0.0000    0.0000       149\n",
      "          37     0.0259    0.3939    0.0486       132\n",
      "          38     0.3893    0.3973    0.3932       146\n",
      "          39     0.0000    0.0000    0.0000       135\n",
      "          40     0.0000    0.0000    0.0000       122\n",
      "          41     0.0000    0.0000    0.0000       133\n",
      "          42     0.0000    0.0000    0.0000       117\n",
      "          43     0.0000    0.0000    0.0000        32\n",
      "          44     0.2440    0.3060    0.2715       134\n",
      "          45     0.0000    0.0000    0.0000       141\n",
      "          46     0.0000    0.0000    0.0000       127\n",
      "          47     0.0000    0.0000    0.0000       109\n",
      "          48     0.3125    0.1064    0.1587       141\n",
      "          49     0.2683    0.1930    0.2245       114\n",
      "          50     0.7204    1.0000    0.8375       134\n",
      "          51     0.0000    0.0000    0.0000       122\n",
      "          52     0.6341    0.3768    0.4727       138\n",
      "          53     0.0000    0.0000    0.0000       141\n",
      "          54     0.0000    0.0000    0.0000       140\n",
      "          55     0.0000    0.0000    0.0000       133\n",
      "          56     0.0000    0.0000    0.0000       135\n",
      "          57     0.0000    0.0000    0.0000       130\n",
      "          58     0.0000    0.0000    0.0000       127\n",
      "          59     0.0000    0.0000    0.0000       107\n",
      "          60     0.0000    0.0000    0.0000       117\n",
      "          61     0.6650    1.0000    0.7988       131\n",
      "          62     0.7083    0.1339    0.2252       127\n",
      "          63     0.0000    0.0000    0.0000       149\n",
      "          64     0.0000    0.0000    0.0000       126\n",
      "          65     0.0000    0.0000    0.0000       126\n",
      "          66     0.2094    1.0000    0.3463       120\n",
      "          67     0.0000    0.0000    0.0000       135\n",
      "          68     0.0000    0.0000    0.0000       126\n",
      "          69     1.0000    0.4609    0.6310       128\n",
      "          70     0.0000    0.0000    0.0000       128\n",
      "          71     0.0000    0.0000    0.0000       122\n",
      "          72     0.0000    0.0000    0.0000       125\n",
      "          73     0.0000    0.0000    0.0000       125\n",
      "\n",
      "    accuracy                         0.1072      9399\n",
      "   macro avg     0.1534    0.1044    0.0872      9399\n",
      "weighted avg     0.1545    0.1072    0.0896      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.947016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoost Classifier</td>\n",
       "      <td>0.874455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>0.882222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost Classifier</td>\n",
       "      <td>0.931908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>0.279817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.274391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB Classifier</td>\n",
       "      <td>0.299394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ComplementNB Classifier</td>\n",
       "      <td>0.107245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349\n",
       "1                   Bagging Classifier  0.947016\n",
       "1             GradientBoost Classifier  0.874455\n",
       "1                   XGBoost Classifier  0.882222\n",
       "1                  CatBoost Classifier  0.931908\n",
       "2                       SVM Classifier  0.279817\n",
       "2                       SGD Classifier  0.274391\n",
       "2             MultinomialNB Classifier  0.299394\n",
       "2              ComplementNB Classifier  0.107245"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "data_3_resampled_NB = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_pred_NB = data_3_resampled_NB.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test,y_pred_NB)\n",
    "\n",
    "report = classification_report(y_test, data_3_resampled_NB.predict(X_test), digits=4)\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_NB.predict(X_test), digits=4))\n",
    "\n",
    "#results_ML_Models = pd.DataFrame()\n",
    "results_data_3_resampled_NB = pd.DataFrame({'Model':['MultinomialNB Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred_NB)]},index={'2'})\n",
    "results_ML_Models = pd.concat([results_ML_Models, results_data_3_resampled_NB])\n",
    "results_ML_Models\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "data_3_resampled_CNB = ComplementNB()\n",
    "data_3_resampled_CNB.fit(X_train, y_train)\n",
    "\n",
    "y_pred_CNB = data_3_resampled_CNB.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test,y_pred_CNB)\n",
    "print(classification_report(y_test, data_3_resampled_CNB.predict(X_test), digits=4))\n",
    "\n",
    "#results_ML_Models = pd.DataFrame()\n",
    "results_data_3_resampled_CNB = pd.DataFrame({'Model':['ComplementNB Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred_CNB)]},index={'2'})\n",
    "results_ML_Models = pd.concat([results_ML_Models, results_data_3_resampled_CNB])\n",
    "results_ML_Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVP-rgVn317Y"
   },
   "source": [
    "# ML Model 8: KNN (Dataset 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40582,
     "status": "ok",
     "timestamp": 1598782640052,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "MRvd88rL317Y",
    "outputId": "b555cde6-5690-485b-c6bf-1a6c23f2e0ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train KNN on dataset 3 : 2.081286907196045\n",
      "Accuracy: 0.8107245451643792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.1135    0.1893       141\n",
      "           1     0.8548    0.8908    0.8724       119\n",
      "           2     0.6625    0.3985    0.4977       133\n",
      "           3     0.8153    1.0000    0.8982       128\n",
      "           4     0.3600    0.1314    0.1925       137\n",
      "           5     0.4154    0.2195    0.2872       123\n",
      "           6     0.5354    0.4492    0.4885       118\n",
      "           7     0.8671    1.0000    0.9288       137\n",
      "           8     0.6220    0.5766    0.5985       137\n",
      "           9     0.8683    1.0000    0.9295       145\n",
      "          10     0.5682    0.6098    0.5882       123\n",
      "          11     0.3833    0.1901    0.2541       121\n",
      "          12     0.4118    0.1892    0.2593       111\n",
      "          13     0.8194    1.0000    0.9008       118\n",
      "          14     0.8302    1.0000    0.9072       132\n",
      "          15     0.9281    1.0000    0.9627       129\n",
      "          16     0.8654    1.0000    0.9278       135\n",
      "          17     0.6700    0.5000    0.5726       134\n",
      "          18     0.4091    0.3750    0.3913       120\n",
      "          19     0.7456    0.9065    0.8182       139\n",
      "          20     0.8366    1.0000    0.9110       128\n",
      "          21     0.7875    0.9618    0.8660       131\n",
      "          22     0.5476    0.5149    0.5308       134\n",
      "          23     0.3542    0.1382    0.1988       123\n",
      "          24     0.8875    0.8987    0.8931        79\n",
      "          25     0.6864    0.8722    0.7682       133\n",
      "          26     1.0000    1.0000    1.0000       125\n",
      "          27     0.4872    0.4419    0.4634       129\n",
      "          28     0.6690    0.7519    0.7080       129\n",
      "          29     1.0000    1.0000    1.0000       133\n",
      "          30     0.8837    1.0000    0.9383       114\n",
      "          31     0.9608    1.0000    0.9800       147\n",
      "          32     0.9429    1.0000    0.9706       132\n",
      "          33     0.9280    1.0000    0.9627       116\n",
      "          34     0.5000    0.5963    0.5439       109\n",
      "          35     0.8356    0.9173    0.8746       133\n",
      "          36     0.8727    0.9664    0.9172       149\n",
      "          37     0.8462    1.0000    0.9167       132\n",
      "          38     0.9932    1.0000    0.9966       146\n",
      "          39     0.9007    0.9407    0.9203       135\n",
      "          40     0.6978    0.7951    0.7433       122\n",
      "          41     1.0000    1.0000    1.0000       133\n",
      "          42     0.8476    0.7607    0.8018       117\n",
      "          43     0.6809    1.0000    0.8101        32\n",
      "          44     0.9781    1.0000    0.9889       134\n",
      "          45     0.7541    0.6525    0.6996       141\n",
      "          46     0.9270    1.0000    0.9621       127\n",
      "          47     0.9561    1.0000    0.9776       109\n",
      "          48     0.9592    1.0000    0.9792       141\n",
      "          49     0.8906    1.0000    0.9421       114\n",
      "          50     1.0000    1.0000    1.0000       134\n",
      "          51     0.9839    1.0000    0.9919       122\n",
      "          52     0.9517    1.0000    0.9753       138\n",
      "          53     1.0000    0.5390    0.7005       141\n",
      "          54     0.9790    1.0000    0.9894       140\n",
      "          55     0.9638    1.0000    0.9815       133\n",
      "          56     0.5682    0.1852    0.2793       135\n",
      "          57     0.9316    0.8385    0.8826       130\n",
      "          58     1.0000    1.0000    1.0000       127\n",
      "          59     0.8843    1.0000    0.9386       107\n",
      "          60     1.0000    1.0000    1.0000       117\n",
      "          61     1.0000    1.0000    1.0000       131\n",
      "          62     0.9769    1.0000    0.9883       127\n",
      "          63     0.9675    1.0000    0.9835       149\n",
      "          64     0.9844    1.0000    0.9921       126\n",
      "          65     0.9921    1.0000    0.9960       126\n",
      "          66     0.9449    1.0000    0.9717       120\n",
      "          67     0.5480    0.7185    0.6218       135\n",
      "          68     1.0000    1.0000    1.0000       126\n",
      "          69     0.9922    1.0000    0.9961       128\n",
      "          70     0.9697    1.0000    0.9846       128\n",
      "          71     0.9919    1.0000    0.9959       122\n",
      "          72     0.8269    0.3440    0.4859       125\n",
      "          73     0.2074    0.6720    0.3170       125\n",
      "\n",
      "    accuracy                         0.8107      9399\n",
      "   macro avg     0.8038    0.8116    0.7946      9399\n",
      "weighted avg     0.8071    0.8107    0.7958      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.947016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoost Classifier</td>\n",
       "      <td>0.874455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>0.882222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost Classifier</td>\n",
       "      <td>0.931908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>0.279817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.274391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB Classifier</td>\n",
       "      <td>0.299394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ComplementNB Classifier</td>\n",
       "      <td>0.107245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.810725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349\n",
       "1                   Bagging Classifier  0.947016\n",
       "1             GradientBoost Classifier  0.874455\n",
       "1                   XGBoost Classifier  0.882222\n",
       "1                  CatBoost Classifier  0.931908\n",
       "2                       SVM Classifier  0.279817\n",
       "2                       SGD Classifier  0.274391\n",
       "2             MultinomialNB Classifier  0.299394\n",
       "2              ComplementNB Classifier  0.107245\n",
       "1                       KNN Classifier  0.810725"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "scores =[]\n",
    "# for k in range(1,50):\n",
    "#     NNH = KNeighborsClassifier(n_neighbors = k, weights = 'uniform', metric='euclidean')\n",
    "#     NNH.fit(X_train, y_train)\n",
    "#     scores.append(NNH.score(X_test, y_test))\n",
    "    \n",
    "# plt.plot(range(1,50),scores)\n",
    "\n",
    "data_3_resampled_KNN = KNeighborsClassifier(n_neighbors= 12 , weights = 'uniform', metric='euclidean')\n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_KNN.fit(X_train,y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train KNN on dataset 3 : {end - start}\")\n",
    "\n",
    "y_pred=data_3_resampled_KNN.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_KNN.predict(X_test), digits=4))\n",
    "\n",
    "#results_ML_Models = pd.DataFrame()\n",
    "results_data_3_resampled_KNN= pd.DataFrame({'Model':['KNN Classifier'], 'accuracy': [metrics.accuracy_score(y_test, y_pred)]},index={'1'})\n",
    "results_ML_Models  = pd.concat([results_ML_Models, results_data_3_resampled_KNN])\n",
    "results_ML_Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IU-5wtKK317b"
   },
   "source": [
    "# ML Model 8: Logestic Regression (Dataset 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36100,
     "status": "ok",
     "timestamp": 1598782682879,
     "user": {
      "displayName": "syed saifi",
      "photoUrl": "",
      "userId": "06701404400650697318"
     },
     "user_tz": -330
    },
    "id": "sXtka3BV317c",
    "outputId": "309e4fa3-dea0-405c-a07e-75884e131582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 46991\n",
      "Number of Labels:  46991\n",
      "Number of Training Samples: 37592\n",
      "Number of Validation Samples: 9399\n",
      "Time Taken To Train Logistic  on dataset 3 : 32.95309853553772\n",
      "Accuracy: 0.41791679965953826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       141\n",
      "           1     0.4336    0.4118    0.4224       119\n",
      "           2     0.6087    0.3158    0.4158       133\n",
      "           3     0.6049    0.3828    0.4689       128\n",
      "           4     0.1475    0.0657    0.0909       137\n",
      "           5     0.3158    0.1463    0.2000       123\n",
      "           6     0.2593    0.0593    0.0966       118\n",
      "           7     0.4444    0.3504    0.3918       137\n",
      "           8     0.2361    0.1241    0.1627       137\n",
      "           9     0.2083    0.2759    0.2374       145\n",
      "          10     0.3208    0.1382    0.1932       123\n",
      "          11     0.0930    0.0331    0.0488       121\n",
      "          12     0.0597    0.1081    0.0769       111\n",
      "          13     0.6500    0.4407    0.5253       118\n",
      "          14     0.4744    0.2803    0.3524       132\n",
      "          15     0.1071    0.0465    0.0649       129\n",
      "          16     0.4023    0.2593    0.3153       135\n",
      "          17     0.0855    0.5597    0.1484       134\n",
      "          18     0.2600    0.1083    0.1529       120\n",
      "          19     0.3065    0.2734    0.2890       139\n",
      "          20     0.3833    0.1797    0.2447       128\n",
      "          21     0.1548    0.1832    0.1678       131\n",
      "          22     0.1616    0.1194    0.1373       134\n",
      "          23     0.1607    0.0732    0.1006       123\n",
      "          24     0.5217    0.3038    0.3840        79\n",
      "          25     0.1394    0.1729    0.1544       133\n",
      "          26     0.5128    0.4800    0.4959       125\n",
      "          27     0.1347    0.2558    0.1765       129\n",
      "          28     0.1031    0.0775    0.0885       129\n",
      "          29     0.5175    1.0000    0.6821       133\n",
      "          30     0.3448    0.4386    0.3861       114\n",
      "          31     0.5102    0.3401    0.4082       147\n",
      "          32     0.4866    0.6894    0.5705       132\n",
      "          33     0.6186    0.5172    0.5634       116\n",
      "          34     0.3636    0.1835    0.2439       109\n",
      "          35     0.3958    0.1429    0.2099       133\n",
      "          36     0.2793    0.2081    0.2385       149\n",
      "          37     0.2062    0.3030    0.2454       132\n",
      "          38     0.7266    0.6370    0.6788       146\n",
      "          39     0.5303    0.5185    0.5243       135\n",
      "          40     0.5690    0.2705    0.3667       122\n",
      "          41     0.4623    0.6917    0.5542       133\n",
      "          42     0.4607    0.3504    0.3981       117\n",
      "          43     0.2214    0.9688    0.3605        32\n",
      "          44     0.3543    0.3358    0.3448       134\n",
      "          45     0.1971    0.1915    0.1942       141\n",
      "          46     0.3731    0.3937    0.3831       127\n",
      "          47     0.3900    0.3578    0.3732       109\n",
      "          48     0.4813    0.5461    0.5116       141\n",
      "          49     0.5812    0.5965    0.5887       114\n",
      "          50     0.8024    1.0000    0.8904       134\n",
      "          51     0.3750    0.2459    0.2970       122\n",
      "          52     0.8166    1.0000    0.8990       138\n",
      "          53     0.7170    0.5390    0.6154       141\n",
      "          54     0.8383    1.0000    0.9121       140\n",
      "          55     0.7591    0.7820    0.7704       133\n",
      "          56     0.0349    0.0222    0.0271       135\n",
      "          57     0.3802    0.3538    0.3665       130\n",
      "          58     0.7560    1.0000    0.8610       127\n",
      "          59     0.1462    0.4112    0.2157       107\n",
      "          60     0.4850    0.6923    0.5704       117\n",
      "          61     0.9357    1.0000    0.9668       131\n",
      "          62     0.5905    0.4882    0.5345       127\n",
      "          63     0.5543    0.3423    0.4232       149\n",
      "          64     0.6811    1.0000    0.8103       126\n",
      "          65     0.6260    0.6508    0.6381       126\n",
      "          66     0.9917    1.0000    0.9959       120\n",
      "          67     0.1154    0.0667    0.0845       135\n",
      "          68     0.5526    1.0000    0.7119       126\n",
      "          69     0.8477    1.0000    0.9176       128\n",
      "          70     0.7209    0.4844    0.5794       128\n",
      "          71     0.8652    1.0000    0.9278       122\n",
      "          72     0.6400    0.2560    0.3657       125\n",
      "          73     0.1111    0.0320    0.0497       125\n",
      "\n",
      "    accuracy                         0.4179      9399\n",
      "   macro avg     0.4284    0.4226    0.4035      9399\n",
      "weighted avg     0.4303    0.4179    0.4046      9399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.949463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier - Weighted</td>\n",
       "      <td>0.949782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost Classifier</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>0.947016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoost Classifier</td>\n",
       "      <td>0.874455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>0.882222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost Classifier</td>\n",
       "      <td>0.931908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>0.279817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.274391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB Classifier</td>\n",
       "      <td>0.299394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ComplementNB Classifier</td>\n",
       "      <td>0.107245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.810725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.417917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  accuracy\n",
       "1                        Random Forest  0.949463\n",
       "1  Random Forest Classifier - Weighted  0.949782\n",
       "1                  Adaboost Classifier  0.082349\n",
       "1                   Bagging Classifier  0.947016\n",
       "1             GradientBoost Classifier  0.874455\n",
       "1                   XGBoost Classifier  0.882222\n",
       "1                  CatBoost Classifier  0.931908\n",
       "2                       SVM Classifier  0.279817\n",
       "2                       SGD Classifier  0.274391\n",
       "2             MultinomialNB Classifier  0.299394\n",
       "2              ComplementNB Classifier  0.107245\n",
       "1                       KNN Classifier  0.810725\n",
       "1                  Logistic Regression  0.417917"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,X = wordTokenizer(data_3_resampled[data_3_resampled.columns[1]])\n",
    "\n",
    "y = np.asarray(data_3_resampled[data_3_resampled.columns[4]])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(\"Number of Labels: \", len(y))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Number of Training Samples:\", len(X_train))\n",
    "print(\"Number of Validation Samples:\", len(X_test))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data_3_resampled_Log = LogisticRegression()\n",
    "\n",
    "start = time.time()\n",
    "data_3_resampled_Log.fit(X_train,y_train)\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "print(f\"Time Taken To Train Logistic  on dataset 3 : {end - start}\")\n",
    "\n",
    "y_pred = data_3_resampled_Log.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, data_3_resampled_Log.predict(X_test), digits=4))\n",
    "\n",
    "#results_ML_Models = pd.DataFrame()\n",
    "results_data_3_resampled_Log= pd.DataFrame({'Model':['Logistic Regression'], 'accuracy': [metrics.accuracy_score(y_test, y_pred)]},index={'1'})\n",
    "results_ML_Models  = pd.concat([results_ML_Models, results_data_3_resampled_Log])\n",
    "results_ML_Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQpesK4L317e"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzdDpmgs317f"
   },
   "source": [
    " \n",
    "- Model Development using Machine Learning models. We have explored 13 machine learning models in Part 5/6 of the source code. The performance is also evaluation. (please refer accuracies, performacne evaluation from above or from interim report for more details): \n",
    "\n",
    "- Machine Learning Models \n",
    "\n",
    "| Model | Test accuracy |\n",
    "| :- | -: |\n",
    "| Random Forest | 0.949463\n",
    "| Random Forest Classifier - Weighted | 0.949782\n",
    "| Adaboost Classifier | 0.082349\n",
    "| Bagging Classifier | 0.947016\n",
    "| GradientBoost Classifier | 0.874455\n",
    "| XGBoost Classifier | 0.882222\n",
    "| CatBoost Classifier | 0.931908\n",
    "| SVM Classifier | 0.279817\n",
    "| SGD Classifier | 0.274391\n",
    "| MultinomialNB Classifier | 0.299394\n",
    "| ComplementNB Classifier | 0.107245\n",
    "| KNN Classifier | 0.810725\n",
    "| Logistic Regression | 0.417917"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNs_xQ-F317i"
   },
   "source": [
    "Part 6/6: Part 6/6 focuses on HyperParameter Tuning for the models explored. Random Forest model seems to be performing well comared to other classifiers. Badding classifier is also performing equivalently."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AUTOMATIC_TICKET_ASSIGNMENT_FINAL_SUBMISSION_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
